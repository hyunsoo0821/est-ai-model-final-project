{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de37fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (8.3.228)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (0.22.1+cu118)\n",
      "Requirement already satisfied: psutil in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (1.35.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from polars->ultralytics) (1.35.2)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from triton==3.3.1->torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: pandas in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pyyaml in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (6.0.3)\n",
      "Requirement already satisfied: pillow in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pyyaml\n",
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940b147",
   "metadata": {},
   "source": [
    "yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ace4b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using preferred classification weight: yolo11m-cls.pt\n",
      "[INFO] 백본 출력 차원=1000 -> 2-클래스용 Linear 헤드 추가.\n",
      "[INFO] 하이브리드 대조 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/30:   0%|          | 0/298 [00:00<?, ?it/s]/tmp/ipykernel_494914/1494063504.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "/tmp/ipykernel_494914/1494063504.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  confidences = torch.tensor(confidences, dtype=torch.float32, device=device)\n",
      "Train Epoch 1/30: 100%|██████████| 298/298 [00:33<00:00,  8.88it/s]\n",
      "Val Epoch 1/30:   0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_494914/1494063504.py:297: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
      "/tmp/ipykernel_494914/1494063504.py:298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  confidences = torch.tensor(confidences, dtype=torch.float32, device=device)\n",
      "Val Epoch 1/30: 100%|██████████| 36/36 [00:06<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 983.1356, Train Acc: 0.6884 | Val Loss: 0.6979, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/30: 100%|██████████| 298/298 [00:28<00:00, 10.42it/s]\n",
      "Val Epoch 2/30: 100%|██████████| 36/36 [00:05<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30] Train Loss: 970.3675, Train Acc: 0.7790 | Val Loss: 0.6981, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/30: 100%|██████████| 298/298 [00:28<00:00, 10.34it/s]\n",
      "Val Epoch 3/30: 100%|██████████| 36/36 [00:06<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30] Train Loss: 953.5927, Train Acc: 0.8004 | Val Loss: 0.6982, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/30: 100%|██████████| 298/298 [00:29<00:00, 10.03it/s]\n",
      "Val Epoch 4/30: 100%|██████████| 36/36 [00:06<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30] Train Loss: 1003.9756, Train Acc: 0.8108 | Val Loss: 0.6985, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/30: 100%|██████████| 298/298 [00:28<00:00, 10.32it/s]\n",
      "Val Epoch 5/30: 100%|██████████| 36/36 [00:06<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30] Train Loss: 1013.9079, Train Acc: 0.8173 | Val Loss: 0.6983, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/30: 100%|██████████| 298/298 [00:36<00:00,  8.17it/s]\n",
      "Val Epoch 6/30: 100%|██████████| 36/36 [00:06<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30] Train Loss: 972.3912, Train Acc: 0.8231 | Val Loss: 0.6986, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/30: 100%|██████████| 298/298 [00:35<00:00,  8.29it/s]\n",
      "Val Epoch 7/30: 100%|██████████| 36/36 [00:06<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30] Train Loss: 1003.6663, Train Acc: 0.8332 | Val Loss: 0.6986, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/30: 100%|██████████| 298/298 [00:32<00:00,  9.05it/s]\n",
      "Val Epoch 8/30: 100%|██████████| 36/36 [00:07<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30] Train Loss: 1004.4058, Train Acc: 0.8328 | Val Loss: 0.6990, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/30: 100%|██████████| 298/298 [00:29<00:00,  9.98it/s]\n",
      "Val Epoch 9/30: 100%|██████████| 36/36 [00:06<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30] Train Loss: 984.8250, Train Acc: 0.8370 | Val Loss: 0.6991, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/30: 100%|██████████| 298/298 [00:29<00:00, 10.16it/s]\n",
      "Val Epoch 10/30: 100%|██████████| 36/36 [00:06<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30] Train Loss: 979.2273, Train Acc: 0.8387 | Val Loss: 0.6990, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/30: 100%|██████████| 298/298 [00:32<00:00,  9.07it/s]\n",
      "Val Epoch 11/30: 100%|██████████| 36/36 [00:06<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30] Train Loss: 1003.5099, Train Acc: 0.8390 | Val Loss: 0.6991, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/30: 100%|██████████| 298/298 [00:29<00:00, 10.16it/s]\n",
      "Val Epoch 12/30: 100%|██████████| 36/36 [00:06<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30] Train Loss: 997.0768, Train Acc: 0.8434 | Val Loss: 0.6996, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/30: 100%|██████████| 298/298 [00:36<00:00,  8.27it/s]\n",
      "Val Epoch 13/30: 100%|██████████| 36/36 [00:06<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30] Train Loss: 986.4629, Train Acc: 0.8413 | Val Loss: 0.6993, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/30: 100%|██████████| 298/298 [00:28<00:00, 10.31it/s]\n",
      "Val Epoch 14/30: 100%|██████████| 36/36 [00:05<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30] Train Loss: 985.6150, Train Acc: 0.8502 | Val Loss: 0.6995, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15/30: 100%|██████████| 298/298 [00:29<00:00, 10.25it/s]\n",
      "Val Epoch 15/30: 100%|██████████| 36/36 [00:06<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30] Train Loss: 979.2452, Train Acc: 0.8498 | Val Loss: 0.6994, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16/30: 100%|██████████| 298/298 [00:27<00:00, 10.75it/s]\n",
      "Val Epoch 16/30: 100%|██████████| 36/36 [00:06<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30] Train Loss: 969.8356, Train Acc: 0.8501 | Val Loss: 0.6997, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17/30: 100%|██████████| 298/298 [00:29<00:00, 10.20it/s]\n",
      "Val Epoch 17/30: 100%|██████████| 36/36 [00:06<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30] Train Loss: 980.1511, Train Acc: 0.8479 | Val Loss: 0.6997, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18/30: 100%|██████████| 298/298 [00:28<00:00, 10.34it/s]\n",
      "Val Epoch 18/30: 100%|██████████| 36/36 [00:06<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30] Train Loss: 993.2946, Train Acc: 0.8488 | Val Loss: 0.6999, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19/30: 100%|██████████| 298/298 [00:29<00:00, 10.16it/s]\n",
      "Val Epoch 19/30: 100%|██████████| 36/36 [00:06<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30] Train Loss: 996.0968, Train Acc: 0.8485 | Val Loss: 0.7001, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20/30: 100%|██████████| 298/298 [00:28<00:00, 10.32it/s]\n",
      "Val Epoch 20/30: 100%|██████████| 36/36 [00:07<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30] Train Loss: 977.8599, Train Acc: 0.8537 | Val Loss: 0.7002, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21/30: 100%|██████████| 298/298 [00:29<00:00, 10.27it/s]\n",
      "Val Epoch 21/30: 100%|██████████| 36/36 [00:06<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30] Train Loss: 965.5407, Train Acc: 0.8587 | Val Loss: 0.7000, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22/30: 100%|██████████| 298/298 [00:28<00:00, 10.38it/s]\n",
      "Val Epoch 22/30: 100%|██████████| 36/36 [00:06<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30] Train Loss: 999.0843, Train Acc: 0.8557 | Val Loss: 0.7001, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23/30: 100%|██████████| 298/298 [00:34<00:00,  8.72it/s]\n",
      "Val Epoch 23/30: 100%|██████████| 36/36 [00:06<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30] Train Loss: 1011.1371, Train Acc: 0.8544 | Val Loss: 0.7002, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24/30: 100%|██████████| 298/298 [00:28<00:00, 10.44it/s]\n",
      "Val Epoch 24/30: 100%|██████████| 36/36 [00:06<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30] Train Loss: 1001.3731, Train Acc: 0.8540 | Val Loss: 0.7002, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25/30: 100%|██████████| 298/298 [00:29<00:00, 10.21it/s]\n",
      "Val Epoch 25/30: 100%|██████████| 36/36 [00:06<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30] Train Loss: 987.2490, Train Acc: 0.8584 | Val Loss: 0.7005, Val Acc: 0.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26/30: 100%|██████████| 298/298 [00:30<00:00,  9.87it/s]\n",
      "Val Epoch 26/30: 100%|██████████| 36/36 [00:06<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30] Train Loss: 975.0358, Train Acc: 0.8558 | Val Loss: 0.7006, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27/30: 100%|██████████| 298/298 [00:29<00:00, 10.24it/s]\n",
      "Val Epoch 27/30: 100%|██████████| 36/36 [00:06<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30] Train Loss: 983.4770, Train Acc: 0.8606 | Val Loss: 0.7006, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28/30: 100%|██████████| 298/298 [00:27<00:00, 10.78it/s]\n",
      "Val Epoch 28/30: 100%|██████████| 36/36 [00:06<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30] Train Loss: 1011.1737, Train Acc: 0.8574 | Val Loss: 0.7005, Val Acc: 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29/30: 100%|██████████| 298/298 [00:29<00:00, 10.12it/s]\n",
      "Val Epoch 29/30: 100%|██████████| 36/36 [00:06<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30] Train Loss: 1004.1487, Train Acc: 0.8585 | Val Loss: 0.7004, Val Acc: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30/30: 100%|██████████| 298/298 [00:27<00:00, 10.69it/s]\n",
      "Val Epoch 30/30: 100%|██████████| 36/36 [00:06<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30] Train Loss: 1002.0744, Train Acc: 0.8577 | Val Loss: 0.7007, Val Acc: 0.2611\n",
      "[INFO] 모델 저장 완료: /workspace/user5/runs/hybrid_contrastive_model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# 0) 데이터 소스\n",
    "# =========================\n",
    "\n",
    "HAPPY_SOURCES_TRAIN = {\n",
    "    \"happy\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "        \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "OTHER_SOURCES_TRAIN = {\n",
    "    \"anger\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "    },\n",
    "    \"panic\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "    },\n",
    "    \"sadness\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "    },\n",
    "}\n",
    "\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": HAPPY_SOURCES_TRAIN,\n",
    "    \"other\": OTHER_SOURCES_TRAIN\n",
    "}\n",
    "\n",
    "HAPPY_SOURCES_VAL = {\n",
    "    \"happy\": {\n",
    "        \"img_dir\": \"/workspace/user4/segmented_output_bisenet/val/happy\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "OTHER_SOURCES_VAL = {\n",
    "    \"anger\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "    },\n",
    "    \"panic\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "    },\n",
    "    \"sadness\": {\n",
    "        \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "        \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "    },\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": HAPPY_SOURCES_VAL,\n",
    "    \"other\": OTHER_SOURCES_VAL\n",
    "}\n",
    "\n",
    "# 클래스 매핑 (2-클래스)\n",
    "CLASS_TO_ID = {\n",
    "    \"happy\": 0,\n",
    "    \"other\": 1,\n",
    "}\n",
    "\n",
    "# 분류 백본 가중치\n",
    "PREFERRED_CLS_WEIGHT = \"yolo11m-cls.pt\"\n",
    "FALLBACK_CLS_WEIGHT = \"yolov8m-cls.pt\"\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = Path(\"/workspace/user5/runs/hybrid_contrastive_model.pt\")\n",
    "SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) 손실 함수들\n",
    "# =========================\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "\n",
    "class WeakLabelWeightedLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(WeakLabelWeightedLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets, confidence_scores):\n",
    "        loss = self.ce_loss(inputs, targets)\n",
    "        weighted_loss = loss * confidence_scores\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) 멀티소스 데이터셋\n",
    "# =========================\n",
    "\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict[str, Dict[str, Dict[str, str]]], transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "\n",
    "        if \"happy\" in sources:\n",
    "            for _, info in sources[\"happy\"].items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    label = CLASS_TO_ID[\"happy\"]\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, label, conf))\n",
    "\n",
    "        if \"other\" in sources:\n",
    "            for _, info in sources[\"other\"].items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    label = CLASS_TO_ID[\"other\"]\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, label, conf))\n",
    "\n",
    "        if len(self.items) == 0:\n",
    "            print(\"[WARN] 데이터가 비어 있습니다. img_dir들을 확인하세요.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, conf, str(img_path)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) 학습 루프\n",
    "# =========================\n",
    "\n",
    "def pick_cls_weight():\n",
    "    def exists_any(name: str) -> bool:\n",
    "        p = Path(name)\n",
    "        return p.exists() or (Path.cwd() / name).exists()\n",
    "    if exists_any(PREFERRED_CLS_WEIGHT):\n",
    "        print(f\"[INFO] Using preferred classification weight: {PREFERRED_CLS_WEIGHT}\")\n",
    "        return PREFERRED_CLS_WEIGHT\n",
    "    elif exists_any(FALLBACK_CLS_WEIGHT):\n",
    "        print(f\"[INFO] Preferred not found. Using fallback classification weight: {FALLBACK_CLS_WEIGHT}\")\n",
    "        return FALLBACK_CLS_WEIGHT\n",
    "    else:\n",
    "        print(\"[WARN] 로컬 분류 가중치를 찾지 못했습니다. 다운로드가 필요할 수 있습니다.\")\n",
    "        return PREFERRED_CLS_WEIGHT\n",
    "\n",
    "\n",
    "def train_custom_hybrid_model():\n",
    "    # 1) 모델 및 디바이스\n",
    "    weight = pick_cls_weight()\n",
    "    yolo_model = YOLO(weight)\n",
    "    backbone = yolo_model.model\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone.to(device)\n",
    "\n",
    "    # 2) 데이터셋/로더\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=transform)\n",
    "    val_dataset = MultiSourceEmotionDataset(VAL_SOURCES, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # 3) 분류 헤드 설정\n",
    "    backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "        raw_out = backbone(dummy)\n",
    "        \n",
    "        # [FIX] 튜플이면 첫 번째 요소(Tensor)만 사용\n",
    "        if isinstance(raw_out, tuple):\n",
    "            raw_out = raw_out[0]\n",
    "            \n",
    "        feat_dim = raw_out.shape[-1]\n",
    "        \n",
    "    backbone.train()\n",
    "\n",
    "    if feat_dim == 2:\n",
    "        classification_head = nn.Identity().to(device)\n",
    "        print(\"[INFO] 백본이 이미 2-클래스 출력입니다. 별도 헤드 없이 진행합니다.\")\n",
    "    else:\n",
    "        classification_head = nn.Linear(feat_dim, 2).to(device)\n",
    "        print(f\"[INFO] 백본 출력 차원={feat_dim} -> 2-클래스용 Linear 헤드 추가.\")\n",
    "\n",
    "    # 4) 손실함수/옵티마이저\n",
    "    criterion_cls = WeakLabelWeightedLoss()\n",
    "    criterion_cont = ContrastiveLoss(margin=2.0)\n",
    "\n",
    "    params = list(backbone.parameters()) + list(classification_head.parameters())\n",
    "    optimizer = optim.Adam(params, lr=1e-4)\n",
    "\n",
    "    # 5) 학습\n",
    "    num_epochs = 30\n",
    "    print(\"[INFO] 하이브리드 대조 학습 시작...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        backbone.train()\n",
    "        classification_head.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total, correct = 0, 0\n",
    "\n",
    "        for images, labels, confidences, _ in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            confidences = torch.tensor(confidences, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Forward\n",
    "            raw_outputs = backbone(images)\n",
    "            # [FIX] 튜플 처리 (학습 루프)\n",
    "            if isinstance(raw_outputs, tuple):\n",
    "                raw_outputs = raw_outputs[0]\n",
    "\n",
    "            logits = classification_head(raw_outputs)\n",
    "\n",
    "            # Loss 1: 분류\n",
    "            loss_cls = criterion_cls(logits, labels, confidences)\n",
    "\n",
    "            # Loss 2: 대조\n",
    "            half = images.size(0) // 2\n",
    "            if half > 0:\n",
    "                emb1, emb2 = raw_outputs[:half], raw_outputs[half:2*half]\n",
    "                lbl1, lbl2 = labels[:half], labels[half:2*half]\n",
    "                contrastive_target = (lbl1 != lbl2).float()\n",
    "                loss_cont = criterion_cont(emb1, emb2, contrastive_target)\n",
    "            else:\n",
    "                loss_cont = torch.tensor(0.0, device=device)\n",
    "\n",
    "            total_loss = loss_cls + 0.5 * loss_cont\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, len(train_loader))\n",
    "        train_acc = correct / max(1, total)\n",
    "\n",
    "        # --- Validation ---\n",
    "        backbone.eval()\n",
    "        classification_head.eval()\n",
    "        val_running, val_total, val_correct = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, confidences, _ in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}/{num_epochs}\"):\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "                confidences = torch.tensor(confidences, dtype=torch.float32, device=device)\n",
    "\n",
    "                raw_outputs = backbone(images)\n",
    "                # [FIX] 튜플 처리 (검증 루프)\n",
    "                if isinstance(raw_outputs, tuple):\n",
    "                    raw_outputs = raw_outputs[0]\n",
    "\n",
    "                logits = classification_head(raw_outputs)\n",
    "\n",
    "                loss_cls = criterion_cls(logits, labels, confidences)\n",
    "                val_running += loss_cls.item()\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_running / max(1, len(val_loader))\n",
    "        val_acc = val_correct / max(1, val_total)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"backbone_state\": backbone.state_dict(),\n",
    "        \"head_state\": classification_head.state_dict(),\n",
    "        \"feat_dim\": feat_dim,\n",
    "    }, SAVE_PATH)\n",
    "    print(f\"[INFO] 모델 저장 완료: {SAVE_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_custom_hybrid_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81352e11",
   "metadata": {},
   "source": [
    "cBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43896ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading ResNet50 weights...\n",
      "[INFO] Start Training for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:35<00:00,  8.26it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8425 | Val Acc: 0.9600 | Val F1: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:33<00:00,  8.87it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9632 | Val Acc: 0.9661 | Val F1: 0.9564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:34<00:00,  8.64it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9715 | Val Acc: 0.9574 | Val F1: 0.9464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:34<00:00,  8.71it/s, loss=0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9784 | Val Acc: 0.9591 | Val F1: 0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:34<00:00,  8.54it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9811 | Val Acc: 0.9478 | Val F1: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:32<00:00,  9.07it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9879 | Val Acc: 0.9478 | Val F1: 0.9352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:38<00:00,  7.80it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9875 | Val Acc: 0.9547 | Val F1: 0.9434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:34<00:00,  8.62it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9877 | Val Acc: 0.9687 | Val F1: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:35<00:00,  8.39it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9899 | Val Acc: 0.9695 | Val F1: 0.9610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:32<00:00,  9.06it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9911 | Val Acc: 0.9687 | Val F1: 0.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:34<00:00,  8.55it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9936 | Val Acc: 0.9661 | Val F1: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 297/297 [00:36<00:00,  8.17it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9925 | Val Acc: 0.9539 | Val F1: 0.9423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 297/297 [00:41<00:00,  7.17it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9942 | Val Acc: 0.9721 | Val F1: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 297/297 [00:34<00:00,  8.71it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9943 | Val Acc: 0.9730 | Val F1: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 297/297 [00:35<00:00,  8.46it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9963 | Val Acc: 0.9774 | Val F1: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 297/297 [00:33<00:00,  8.99it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9986 | Val Acc: 0.9739 | Val F1: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 297/297 [00:34<00:00,  8.60it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9974 | Val Acc: 0.9765 | Val F1: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 297/297 [00:33<00:00,  8.78it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9976 | Val Acc: 0.9704 | Val F1: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 297/297 [00:33<00:00,  8.84it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9981 | Val Acc: 0.9652 | Val F1: 0.9563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 297/297 [00:34<00:00,  8.73it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9985 | Val Acc: 0.9669 | Val F1: 0.9582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 297/297 [00:34<00:00,  8.68it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9983 | Val Acc: 0.9687 | Val F1: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 297/297 [00:33<00:00,  8.88it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9993 | Val Acc: 0.9739 | Val F1: 0.9666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30: 100%|██████████| 297/297 [00:35<00:00,  8.42it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9997 | Val Acc: 0.9704 | Val F1: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/30: 100%|██████████| 297/297 [00:33<00:00,  8.81it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9994 | Val Acc: 0.9721 | Val F1: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/30: 100%|██████████| 297/297 [00:32<00:00,  9.22it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9995 | Val Acc: 0.9721 | Val F1: 0.9645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/30: 100%|██████████| 297/297 [00:39<00:00,  7.47it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9999 | Val Acc: 0.9756 | Val F1: 0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27/30: 100%|██████████| 297/297 [00:34<00:00,  8.69it/s, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9998 | Val Acc: 0.9748 | Val F1: 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28/30: 100%|██████████| 297/297 [00:34<00:00,  8.69it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9994 | Val Acc: 0.9634 | Val F1: 0.9541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29/30: 100%|██████████| 297/297 [00:34<00:00,  8.65it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9999 | Val Acc: 0.9687 | Val F1: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30/30: 100%|██████████| 297/297 [00:32<00:00,  9.03it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9997 | Val Acc: 0.9695 | Val F1: 0.9614\n",
      "[Done] Model Saved: /workspace/user5/runs/cbam_resnet_emotion_model.pt\n",
      "[INFO] Graph Saved: /workspace/user5/runs/results.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"cbam_resnet_emotion_model.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results.csv\"\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 시각화\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        # [수정] train/acc 컬럼 추가\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            row = [data.get(h, \"\") for h in self.header]\n",
    "            writer.writerow(row)\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Loss Curve (Train vs Val)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy Curve (Train vs Val) -> [수정] Train Acc 추가\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 3. Detailed Metrics (Precision, Recall, F1)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics Detailed\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. Learning Rate\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results.png\")\n",
    "        print(f\"[INFO] Graph Saved: {save_dir / 'results.png'}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) CBAM Module & Model\n",
    "# =========================\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out\n",
    "\n",
    "class ResNetCBAM(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNetCBAM, self).__init__()\n",
    "        # Load Pretrained ResNet50\n",
    "        print(\"[INFO] Loading ResNet50 weights...\")\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # 마지막 FC 제거\n",
    "        self.feat_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # CBAM Block (Layer4 [2048] 뒤에 적용)\n",
    "        self.cbam = CBAMBlock(in_planes=2048)\n",
    "        \n",
    "        # Projection Head for Contrastive Learning\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128) # Embedding Size\n",
    "        )\n",
    "        \n",
    "        # Final Classifier\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ResNet Feature Extraction\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        \n",
    "        # Apply CBAM Attention\n",
    "        x = self.cbam(x)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Projection & Classify\n",
    "        embeddings = self.projection(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # 벡터 정규화 (Loss 폭발 방지)\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # 데이터 증강\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = ResNetCBAM(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.2)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        \n",
    "        # [추가] Train Accuracy 계산을 위한 변수\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            # Loss 1: Classification\n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            # Loss 2: Contrastive\n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            # Total Loss\n",
    "            total_loss = loss_c + 0.5 * loss_ct\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            # [추가] Train Accuracy 실시간 계산\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Train Metrics 계산\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                \n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        # Val Metrics\n",
    "        val_acc = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item()\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # CSV Log\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\",\n",
    "            \"train/acc\": f\"{train_acc:.4f}\", # [추가] Train Acc\n",
    "            \"train/loss_cls\": f\"{r_cls/len(train_loader):.4f}\",\n",
    "            \"train/loss_cont\": f\"{r_cont/len(train_loader):.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\",\n",
    "            \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\",\n",
    "            \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\",\n",
    "            \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        \n",
    "        # 콘솔 출력\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "    # Save & Plot\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from timm) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from timm) (0.22.1+cu118)\n",
      "Requirement already satisfied: pyyaml in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from timm) (1.1.4)\n",
      "Requirement already satisfied: safetensors in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: shellingham in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: anyio in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: certifi in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm) (1.3.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch->timm) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from triton==3.3.1->torch->timm) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torchvision->timm) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torchvision->timm) (12.0.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from typer-slim->huggingface_hub->timm) (8.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1383e",
   "metadata": {},
   "source": [
    "mobile vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43e3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading MobileViT-S via timm library...\n",
      "[INFO] Start Training for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/20: 100%|██████████| 297/297 [00:42<00:00,  7.02it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8987 | Val Acc: 0.9391 | Val F1: 0.9246\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/20: 100%|██████████| 297/297 [00:36<00:00,  8.03it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9680 | Val Acc: 0.9582 | Val F1: 0.9474\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/20: 100%|██████████| 297/297 [00:37<00:00,  7.89it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9774 | Val Acc: 0.9626 | Val F1: 0.9528\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/20: 100%|██████████| 297/297 [00:36<00:00,  8.03it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9832 | Val Acc: 0.9721 | Val F1: 0.9638\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/20: 100%|██████████| 297/297 [00:37<00:00,  7.90it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9856 | Val Acc: 0.9739 | Val F1: 0.9662\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/20: 100%|██████████| 297/297 [00:37<00:00,  8.00it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9874 | Val Acc: 0.9582 | Val F1: 0.9476\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/20: 100%|██████████| 297/297 [00:37<00:00,  7.98it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9897 | Val Acc: 0.9495 | Val F1: 0.9373\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/20: 100%|██████████| 297/297 [00:37<00:00,  8.02it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9922 | Val Acc: 0.9652 | Val F1: 0.9559\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/20: 100%|██████████| 297/297 [00:37<00:00,  7.97it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9928 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/20: 100%|██████████| 297/297 [00:39<00:00,  7.51it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9941 | Val Acc: 0.9713 | Val F1: 0.9634\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/20: 100%|██████████| 297/297 [00:37<00:00,  7.84it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9954 | Val Acc: 0.9748 | Val F1: 0.9674\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/20: 100%|██████████| 297/297 [00:37<00:00,  7.87it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9979 | Val Acc: 0.9643 | Val F1: 0.9549\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/20: 100%|██████████| 297/297 [00:37<00:00,  7.90it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9975 | Val Acc: 0.9695 | Val F1: 0.9612\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_013.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/20: 100%|██████████| 297/297 [00:38<00:00,  7.69it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9973 | Val Acc: 0.9704 | Val F1: 0.9623\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/20: 100%|██████████| 297/297 [00:37<00:00,  7.91it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9985 | Val Acc: 0.9661 | Val F1: 0.9571\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/20: 100%|██████████| 297/297 [00:36<00:00,  8.07it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9987 | Val Acc: 0.9661 | Val F1: 0.9571\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/20: 100%|██████████| 297/297 [00:37<00:00,  7.99it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9988 | Val Acc: 0.9695 | Val F1: 0.9612\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_017.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/20: 100%|██████████| 297/297 [00:38<00:00,  7.81it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9993 | Val Acc: 0.9695 | Val F1: 0.9612\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_018.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/20: 100%|██████████| 297/297 [00:38<00:00,  7.71it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9993 | Val Acc: 0.9695 | Val F1: 0.9612\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/20: 100%|██████████| 297/297 [00:37<00:00,  7.83it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9992 | Val Acc: 0.9704 | Val F1: 0.9624\n",
      "[INFO] Validation preview saved: /workspace/user5/runs/val_previews/val_preview_epoch_020.png\n",
      "[Done] Model Saved: /workspace/user5/runs/emotion_vis_model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_vis_model.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"} # 소문자로 변경 (이미지 스타일 맞춤)\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            row = [data.get(h, \"\") for h in self.header]\n",
    "            writer.writerow(row)\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 수정] 시각화 함수 (확률 표시 & YOLO 스타일)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16):\n",
    "    \"\"\"\n",
    "    이미지와 함께 '클래스명 확률' (예: happy 1.0)을 표시하고\n",
    "    테두리를 그리는 스타일로 저장합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 데이터 1배치 가져오기\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    lbls = lbls.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        # Softmax를 통해 확률(Score) 계산\n",
    "        probs = F.softmax(logits, dim=1) \n",
    "        # 가장 높은 확률값과 해당 인덱스 추출\n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    # 정규화 해제 (Un-normalization)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    # Grid 이미지 생성 (matplotlib 사용)\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    # axes가 1차원 배열이거나 단일 객체일 경우 처리\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # 예측값과 실제값 정보\n",
    "            pred_idx = preds[i].item()\n",
    "            true_idx = lbls[i].item()\n",
    "            score = confidences[i].item() # 확률 (0.0 ~ 1.0)\n",
    "            \n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            \n",
    "            # 색상 설정 (Happy: 파랑, Other: 청록색/Cyan - 요청하신 이미지 스타일)\n",
    "            if pred_idx == 0: # Happy\n",
    "                box_color = 'blue'\n",
    "            else: # Other\n",
    "                box_color = 'cyan'\n",
    "            \n",
    "            # 1. 이미지 표시\n",
    "            ax.imshow(img_np)\n",
    "            \n",
    "            # 2. 테두리 그리기 (Spines 색상 변경)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3) # 테두리 두께\n",
    "            \n",
    "            # 3. 텍스트 라벨 (예: happy 1.0)\n",
    "            label_text = f\"{pred_class_name} {score:.1f}\"\n",
    "            \n",
    "            # 텍스트 박스 배경 (좌측 상단)\n",
    "            # 이미지 좌표계: (0,0)이 좌측 상단입니다. \n",
    "            ax.text(\n",
    "                5, 20, # x, y 위치 (픽셀 단위, 약간 안쪽으로)\n",
    "                label_text,\n",
    "                color='white',\n",
    "                fontsize=11,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2)\n",
    "            )\n",
    "            \n",
    "            # (옵션) 틀린 경우 표시를 원하면 아래 주석 해제\n",
    "            # if pred_idx != true_idx:\n",
    "            #     ax.text(5, 50, \"WRONG\", color='red', fontweight='bold')\n",
    "\n",
    "        ax.axis('off') # 축 눈금 숨기기 (테두리는 위에서 설정함)\n",
    "        \n",
    "        # axis('off')를 하면 테두리(spine)도 사라지므로, \n",
    "        # 테두리를 남기고 눈금만 없애려면 아래와 같이 해야 합니다.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # 다시 테두리 켜기 (matplotlib 버전에 따라 axis('off')가 테두리까지 끄는 경우 대비)\n",
    "        ax.spines['top'].set_visible(True)\n",
    "        ax.spines['right'].set_visible(True)\n",
    "        ax.spines['bottom'].set_visible(True)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved: {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: Robust Lightweight Model\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        \n",
    "        self.use_timm = False\n",
    "        in_features = 960 # 기본값\n",
    "\n",
    "        try:\n",
    "            import timm\n",
    "            print(\"[INFO] Loading MobileViT-S via timm library...\")\n",
    "            self.backbone = timm.create_model('mobilevit_s', pretrained=True, num_classes=0)\n",
    "            in_features = self.backbone.num_features\n",
    "            self.use_timm = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load MobileViT via timm: {e}\")\n",
    "\n",
    "        if not self.use_timm:\n",
    "            print(\"[INFO] Falling back to MobileNetV3-Large.\")\n",
    "            try:\n",
    "                weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V1\n",
    "                self.backbone = models.mobilenet_v3_large(weights=weights)\n",
    "            except AttributeError:\n",
    "                self.backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "            \n",
    "            if hasattr(self.backbone.classifier, '0'):\n",
    "                in_features = self.backbone.classifier[0].in_features\n",
    "            else:\n",
    "                in_features = 960\n",
    "\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU() if hasattr(nn, 'SiLU') else nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        embeddings = self.projection(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            total_loss = loss_c + 0.5 * loss_ct\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        # [수정됨] 확률값(Score)과 함께 저장\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae82328",
   "metadata": {},
   "source": [
    "mobilevit teamerature 하이퍼파라미터 추가후 스케일링해서 세부화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c811f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading MobileViT-S via timm library...\n",
      "[INFO] Start Training for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:36<00:00,  8.03it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8936 | Val Acc: 0.9443 | Val F1: 0.9310\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:38<00:00,  7.64it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9668 | Val Acc: 0.9669 | Val F1: 0.9575\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:37<00:00,  7.94it/s, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9762 | Val Acc: 0.9243 | Val F1: 0.9084\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:38<00:00,  7.76it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9826 | Val Acc: 0.9661 | Val F1: 0.9566\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:37<00:00,  7.89it/s, loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9845 | Val Acc: 0.9643 | Val F1: 0.9549\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:37<00:00,  7.86it/s, loss=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9886 | Val Acc: 0.9774 | Val F1: 0.9709\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:36<00:00,  8.06it/s, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9905 | Val Acc: 0.9669 | Val F1: 0.9581\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:37<00:00,  7.85it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9921 | Val Acc: 0.9608 | Val F1: 0.9506\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:36<00:00,  8.07it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9920 | Val Acc: 0.9652 | Val F1: 0.9560\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:37<00:00,  7.93it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9931 | Val Acc: 0.9617 | Val F1: 0.9518\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30:  59%|█████▉    | 176/297 [00:22<00:15,  7.81it/s, loss=0.21] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 480\u001b[0m\n\u001b[1;32m    477\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 425\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    423\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 425\u001b[0m r_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m r_cls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_c\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    427\u001b[0m r_cont \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_ct\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_vis_model.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            row = [data.get(h, \"\") for h in self.header]\n",
    "            writer.writerow(row)\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    \"\"\"\n",
    "    temperature: 이 값을 높일수록(예: 3.0, 5.0) 확률 분포가 퍼져서 0.6, 0.7 등의 값이 나옵니다.\n",
    "                 기본값 1.0일 때는 0.99, 1.0 처럼 극단적인 값이 나옵니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    lbls = lbls.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        \n",
    "        # [중요] Temperature Scaling: Logit 값을 나누어 분포를 완만하게 만듦\n",
    "        scaled_logits = logits / temperature \n",
    "        \n",
    "        probs = F.softmax(scaled_logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            \n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            \n",
    "            # 색상 설정 (Happy: Blue, Other: Cyan)\n",
    "            if pred_idx == 0: \n",
    "                box_color = 'blue'\n",
    "            else: \n",
    "                box_color = 'cyan'\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            \n",
    "            # 테두리 그리기\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "            \n",
    "            # 텍스트 표시\n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            \n",
    "            ax.text(\n",
    "                5, 20,\n",
    "                label_text,\n",
    "                color='white',\n",
    "                fontsize=11,\n",
    "                fontweight='bold',\n",
    "                bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2)\n",
    "            )\n",
    "\n",
    "        ax.axis('off') \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.spines['top'].set_visible(True)\n",
    "        ax.spines['right'].set_visible(True)\n",
    "        ax.spines['bottom'].set_visible(True)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: Robust Lightweight Model\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        \n",
    "        self.use_timm = False\n",
    "        in_features = 960 # 기본값\n",
    "\n",
    "        # timm 라이브러리 시도\n",
    "        try:\n",
    "            import timm\n",
    "            print(\"[INFO] Loading MobileViT-S via timm library...\")\n",
    "            self.backbone = timm.create_model('mobilevit_s', pretrained=True, num_classes=0)\n",
    "            in_features = self.backbone.num_features\n",
    "            self.use_timm = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load MobileViT via timm: {e}\")\n",
    "\n",
    "        # Fallback: MobileNetV3\n",
    "        if not self.use_timm:\n",
    "            print(\"[INFO] Falling back to MobileNetV3-Large.\")\n",
    "            try:\n",
    "                weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V1\n",
    "                self.backbone = models.mobilenet_v3_large(weights=weights)\n",
    "            except AttributeError:\n",
    "                self.backbone = models.mobilenet_v3_large(pretrained=True)\n",
    "            \n",
    "            if hasattr(self.backbone.classifier, '0'):\n",
    "                in_features = self.backbone.classifier[0].in_features\n",
    "            else:\n",
    "                in_features = 960\n",
    "\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU() if hasattr(nn, 'SiLU') else nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        embeddings = self.projection(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            total_loss = loss_c + 0.5 * loss_ct\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        # [중요] Temperature = 3.0 적용 (분포를 다양하게 만듦)\n",
    "        # 만약 더 분포를 넓히고 싶으면 4.0, 5.0으로 올리세요.\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce37107",
   "metadata": {},
   "source": [
    "yolo모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d996eed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading YOLOv8m-cls (Medium) backbone...\n",
      "[INFO] Backbone Feature Size: 1280\n",
      "[INFO] Start Training with YOLOv8m-cls for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:36<00:00,  8.16it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8207 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:37<00:00,  7.90it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8775 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:37<00:00,  8.00it/s, loss=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8897 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:34<00:00,  8.50it/s, loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8983 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:34<00:00,  8.58it/s, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9047 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:34<00:00,  8.67it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9100 | Val Acc: 0.2602 | Val F1: 0.2065\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs/val_previews_yolo/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:34<00:00,  8.51it/s, loss=0.358]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 460\u001b[0m\n\u001b[1;32m    457\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 426\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    423\u001b[0m all_preds, all_labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, lbls, _ \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m    427\u001b[0m         imgs, lbls \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), lbls\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    428\u001b[0m         _, logits \u001b[38;5;241m=\u001b[39m model(imgs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# YOLO 라이브러리\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except ImportError:\n",
    "    raise ImportError(\"YOLO 모델을 사용하려면 ultralytics 라이브러리가 필요합니다. 'pip install ultralytics'를 실행해주세요.\")\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_vis_model_yolo.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_yolo.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_yolo\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (YOLO-M)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_yolo.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        scaled_logits = logits / temperature \n",
    "        probs = F.softmax(scaled_logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            \n",
    "            if pred_idx == 0: \n",
    "                box_color = 'blue'\n",
    "            else: \n",
    "                box_color = 'cyan'\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "            \n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "\n",
    "        ax.axis('off') \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: YOLOv8m-cls 기반 모델 (최종 수정 버전)\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        \n",
    "        print(\"[INFO] Loading YOLOv8m-cls (Medium) backbone...\")\n",
    "        yolo_temp = YOLO('yolov8m-cls.pt')\n",
    "        self.backbone = yolo_temp.model\n",
    "        \n",
    "        head = self.backbone.model[-1]\n",
    "        \n",
    "        if hasattr(head, 'linear') and isinstance(head.linear, nn.Linear):\n",
    "            in_features = head.linear.in_features\n",
    "            head.linear = nn.Identity()\n",
    "        else:\n",
    "            print(\"[WARN] Unknown head structure, using default 1280 features.\")\n",
    "            in_features = 1280\n",
    "            head.linear = nn.Identity()\n",
    "            \n",
    "        print(f\"[INFO] Backbone Feature Size: {in_features}\")\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # [중요 수정] Eval 모드 등에서 튜플이 반환되는 경우 처리\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        \n",
    "        # 리스트 반환 처리 (YOLO 일반적 동작)\n",
    "        if isinstance(features, list):\n",
    "            features = features[-1]\n",
    "            \n",
    "        # Shape 처리\n",
    "        if features.dim() == 4:\n",
    "            features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "            features = features.flatten(1)\n",
    "        elif features.dim() > 2:\n",
    "             features = features.reshape(features.size(0), -1)\n",
    "\n",
    "        embeddings = self.projection(features)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with YOLOv8m-cls for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() \n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            total_loss = loss_c + 0.5 * loss_ct\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b51b2",
   "metadata": {},
   "source": [
    "yolo11m_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853620c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading YOLO11m-cls (Medium) backbone...\n",
      "[INFO] Backbone Feature Size: 1280\n",
      "[INFO] Start Training with YOLO11m-cls for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:33<00:00,  8.83it/s, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.7937 | Val Acc: 0.2602 | Val F1: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:33<00:00,  8.90it/s, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8429 | Val Acc: 0.2602 | Val F1: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:32<00:00,  9.03it/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8536 | Val Acc: 0.2602 | Val F1: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30:  30%|██▉       | 88/297 [00:12<00:29,  7.11it/s, loss=0.504]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 441\u001b[0m\n\u001b[1;32m    438\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 371\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    368\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    370\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, lbls, confs \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m    372\u001b[0m     imgs, lbls, confs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), lbls\u001b[38;5;241m.\u001b[39mto(device), confs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    374\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# YOLO 라이브러리\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except ImportError:\n",
    "    raise ImportError(\"YOLO 모델을 사용하려면 ultralytics 라이브러리가 필요합니다. 'pip install -U ultralytics'를 실행해주세요.\")\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4 # Loss가 너무 크면 1e-5로 줄여보세요\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_yolo11\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_yolo11.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_yolo11.csv\"\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_yolo11\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로 (기존과 동일)\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (YOLO11-M)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_yolo11.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        scaled_logits = logits / temperature \n",
    "        probs = F.softmax(scaled_logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray): axes = axes.flatten()\n",
    "    else: axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            box_color = 'blue' if pred_idx == 0 else 'cyan'\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "            ax.text(5, 20, f\"{pred_class_name} {score:.2f}\", color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "        ax.axis('off') \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: YOLO11-M (핵심 변경 부분)\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        \n",
    "        # [변경] YOLO11 로드\n",
    "        print(\"[INFO] Loading YOLO11m-cls (Medium) backbone...\")\n",
    "        try:\n",
    "            # yolo11m-cls.pt가 없으면 자동 다운로드 됩니다.\n",
    "            yolo_temp = YOLO('yolo11m-cls.pt') \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] YOLO11 로드 실패. ultralytics 버전을 확인하세요: {e}\")\n",
    "            raise e\n",
    "            \n",
    "        self.backbone = yolo_temp.model\n",
    "        \n",
    "        # Head 교체 로직\n",
    "        head = self.backbone.model[-1]\n",
    "        \n",
    "        # YOLO11도 구조는 비슷하지만 안전하게 속성 확인\n",
    "        if hasattr(head, 'linear') and isinstance(head.linear, nn.Linear):\n",
    "            in_features = head.linear.in_features\n",
    "            head.linear = nn.Identity()\n",
    "        elif hasattr(head, 'fc'): # 혹시 fc를 쓰는 경우\n",
    "            in_features = head.fc.in_features\n",
    "            head.fc = nn.Identity()\n",
    "        else:\n",
    "            print(\"[WARN] Unknown head structure, using default 1280 features.\")\n",
    "            in_features = 1280 # YOLO Medium급은 보통 1024~1280\n",
    "            if hasattr(head, 'linear'): head.linear = nn.Identity()\n",
    "\n",
    "        print(f\"[INFO] Backbone Feature Size: {in_features}\")\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # [중요] YOLO 출력 예외 처리 (Tuple, List 등)\n",
    "        if isinstance(features, tuple):\n",
    "            features = features[0]\n",
    "        \n",
    "        if isinstance(features, list):\n",
    "            features = features[-1]\n",
    "            \n",
    "        if features.dim() == 4:\n",
    "            features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "            features = features.flatten(1)\n",
    "        elif features.dim() > 2:\n",
    "             features = features.reshape(features.size(0), -1)\n",
    "\n",
    "        embeddings = self.projection(features)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss & Train\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # YOLO11은 224x224가 기본인 경우가 많음 (속도 향상)\n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with YOLO11m-cls for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() \n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            # Loss 가중치 조정 (Class loss에 더 집중)\n",
    "            total_loss = loss_c + 0.1 * loss_ct \n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b82d2",
   "metadata": {},
   "source": [
    "efficient net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b6f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading EfficientNet-B4 (Recommended for Classification)...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /home/user5/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74.5M/74.5M [00:00<00:00, 119MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EfficientNet-B4 Feature Size: 1792\n",
      "[INFO] Start Training with EfficientNet-B4 for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:58<00:00,  5.04it/s, acc=0.89, loss=0.405] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8898 | Val Acc: 0.9382 | Val F1: 0.9230\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:55<00:00,  5.40it/s, acc=0.955, loss=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9555 | Val Acc: 0.9530 | Val F1: 0.9410\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:55<00:00,  5.40it/s, acc=0.967, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9666 | Val Acc: 0.9521 | Val F1: 0.9399\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.971, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9715 | Val Acc: 0.9608 | Val F1: 0.9507\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.977, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9771 | Val Acc: 0.9617 | Val F1: 0.9517\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.98, loss=0.273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9802 | Val Acc: 0.9617 | Val F1: 0.9518\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.981, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9811 | Val Acc: 0.9704 | Val F1: 0.9624\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.984, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9837 | Val Acc: 0.9582 | Val F1: 0.9476\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.987, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9873 | Val Acc: 0.9547 | Val F1: 0.9435\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.987, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9871 | Val Acc: 0.9652 | Val F1: 0.9560\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.989, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9886 | Val Acc: 0.9721 | Val F1: 0.9646\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.992, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9917 | Val Acc: 0.9617 | Val F1: 0.9520\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.993, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9928 | Val Acc: 0.9626 | Val F1: 0.9528\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_013.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.992, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9923 | Val Acc: 0.9704 | Val F1: 0.9627\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.995, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9952 | Val Acc: 0.9782 | Val F1: 0.9721\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.995, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9946 | Val Acc: 0.9730 | Val F1: 0.9659\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.995, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9948 | Val Acc: 0.9739 | Val F1: 0.9668\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_017.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.995, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9952 | Val Acc: 0.9669 | Val F1: 0.9583\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_018.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, acc=0.997, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9973 | Val Acc: 0.9721 | Val F1: 0.9646\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_efficientnet/val_previews_effb4/val_preview_epoch_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30:  10%|█         | 31/297 [00:06<00:56,  4.68it/s, acc=1, loss=0.202]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 449\u001b[0m\n\u001b[1;32m    446\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 449\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 396\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    394\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 396\u001b[0m r_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m r_cls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_c\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    398\u001b[0m r_cont \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_ct\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "# EfficientNet은 초기 학습률이 너무 높으면 튈 수 있어 1e-4 유지\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_efficientnet\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effb4.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effb4.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_effb4\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (EfficientNet-B4)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_effb4.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        scaled_logits = logits / temperature \n",
    "        probs = F.softmax(scaled_logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            \n",
    "            if pred_idx == 0: box_color = 'blue'\n",
    "            else: box_color = 'cyan'\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "            \n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "\n",
    "        ax.axis('off') \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: EfficientNet-B4 (Classification Specialist)\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        \n",
    "        print(\"[INFO] Loading EfficientNet-B4 (Recommended for Classification)...\")\n",
    "        # ImageNet V1 가중치를 사용하여 초기 Loss를 낮춤\n",
    "        weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b4(weights=weights)\n",
    "        \n",
    "        # EfficientNet-B4의 마지막 layer는 'classifier'라는 이름의 Sequential입니다.\n",
    "        # 그 안의 두 번째 요소(Linear)의 input features를 가져옵니다.\n",
    "        # 구조: Sequential(Dropout, Linear)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        # 기존 Classifier 제거 (Feature Extractor로만 사용)\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        print(f\"[INFO] EfficientNet-B4 Feature Size: {in_features}\")\n",
    "\n",
    "        # Contrastive Learning을 위한 투영층 (Projection Head)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "        \n",
    "        # 최종 분류기 (Classifier)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Backbone (EfficientNet)\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # 2. Projection (Contrastive용 임베딩)\n",
    "        embeddings = self.projection(features)\n",
    "        \n",
    "        # 3. Classification (최종 예측)\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # EfficientNet은 입력 크기에 민감합니다. 256x256 이상 권장.\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), \n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # [모델 초기화] EfficientNet-B4\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    criterion_cls = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with EfficientNet-B4 for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() \n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embs, logits = model(imgs)\n",
    "            \n",
    "            # 1. Classification Loss (Label Smoothing + Confidence)\n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "            \n",
    "            # 2. Contrastive Loss (Self-Supervised)\n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "            \n",
    "            # [중요] Loss가 너무 크면 Contrastive 비율을 낮춥니다 (0.5 -> 0.1)\n",
    "            total_loss = loss_c + 0.1 * loss_ct\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=total_loss.item(), acc=train_correct/train_total)\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4e3926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading EfficientNet-B4 (Improved Structure)...\n",
      "[INFO] EfficientNet-B4 Feature Size: 1792\n",
      "[INFO] Start Training with Improved EfficientNet-B4 for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [01:00<00:00,  4.92it/s, acc=0.864, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8644 | Val Acc: 0.9487 | Val F1: 0.9354\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.96, loss=0.0323] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9600 | Val Acc: 0.9608 | Val F1: 0.9503\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.972, loss=0.0689] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9724 | Val Acc: 0.9530 | Val F1: 0.9411\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.979, loss=0.0245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9792 | Val Acc: 0.9617 | Val F1: 0.9515\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.981, loss=0.131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9810 | Val Acc: 0.9600 | Val F1: 0.9495\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.987, loss=0.00776] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9870 | Val Acc: 0.9634 | Val F1: 0.9536\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.988, loss=0.0252] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9878 | Val Acc: 0.9661 | Val F1: 0.9571\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, acc=0.99, loss=0.0765]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9898 | Val Acc: 0.9695 | Val F1: 0.9611\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.993, loss=0.00393] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9931 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.992, loss=0.0359]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9924 | Val Acc: 0.9730 | Val F1: 0.9655\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.995, loss=0.018]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9948 | Val Acc: 0.9661 | Val F1: 0.9570\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.995, loss=0.027]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9946 | Val Acc: 0.9652 | Val F1: 0.9560\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, acc=0.995, loss=0.00397] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9953 | Val Acc: 0.9652 | Val F1: 0.9561\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_013.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.997, loss=0.00123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9971 | Val Acc: 0.9687 | Val F1: 0.9602\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.996, loss=0.0332]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9964 | Val Acc: 0.9721 | Val F1: 0.9645\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.998, loss=0.0017]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9977 | Val Acc: 0.9704 | Val F1: 0.9625\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.997, loss=0.00116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9967 | Val Acc: 0.9713 | Val F1: 0.9634\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_017.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.998, loss=0.0163]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9981 | Val Acc: 0.9730 | Val F1: 0.9656\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_018.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.999, loss=0.00297] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9987 | Val Acc: 0.9704 | Val F1: 0.9624\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.999, loss=0.00639] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9985 | Val Acc: 0.9713 | Val F1: 0.9634\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_020.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.998, loss=0.000769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9978 | Val Acc: 0.9748 | Val F1: 0.9677\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_021.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, acc=0.998, loss=0.00439] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9983 | Val Acc: 0.9739 | Val F1: 0.9667\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_022.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.999, loss=0.000241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9985 | Val Acc: 0.9730 | Val F1: 0.9656\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_023.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.998, loss=0.000539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9980 | Val Acc: 0.9748 | Val F1: 0.9677\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_024.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.999, loss=0.00212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.9986 | Val Acc: 0.9721 | Val F1: 0.9646\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_025.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/30:  10%|▉         | 29/297 [00:06<00:59,  4.50it/s, acc=0.999, loss=0.000155]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 451\u001b[0m\n\u001b[1;32m    448\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 396\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    394\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 396\u001b[0m r_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m r_cls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_c\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    398\u001b[0m r_cont \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_ct\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_effb4_imp\") # 경로 이름 변경 (improved)\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effb4_imp.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effb4_imp.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_effb4_imp\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (EfficientNet-B4 Improved)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_effb4_imp.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward 수정으로 반환값이 (embeddings, logits)가 됨\n",
    "        _, logits = model(imgs)\n",
    "        # Temperature Scaling은 그대로 유지 (학습이 잘 되면 이 값에서도 분포가 넓어질 것임)\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=1)\n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "\n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "\n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "\n",
    "            if pred_idx == 0: box_color = 'blue'\n",
    "            else: box_color = 'cyan'\n",
    "\n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    # weak 데이터에 대한 신뢰도 가중치는 그대로 유지\n",
    "                    conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: 구조 개선된 EfficientNet-B4\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "\n",
    "        print(\"[INFO] Loading EfficientNet-B4 (Improved Structure)...\")\n",
    "        weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b4(weights=weights)\n",
    "\n",
    "        # EfficientNet-B4의 최종 특징 맵 차원 (보통 1792)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity() # 기존 분류기 제거\n",
    "\n",
    "        print(f\"[INFO] EfficientNet-B4 Feature Size: {in_features}\")\n",
    "\n",
    "        # [구조 변경 1] Projection Head (Contrastive Loss용, 128차원 압축)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "\n",
    "        # [구조 변경 2] Classifier (분류용, 원본 특징 직접 사용)\n",
    "        # 입력 차원이 Projection 출력이 아닌 Backbone 출력(in_features)이 됨\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Backbone 특징 추출 (풍부한 정보)\n",
    "        features = self.backbone(x) # [Batch, in_features]\n",
    "\n",
    "        # 2. 두 갈래로 분기\n",
    "        embeddings = self.projection(features) # Contrastive용 임베딩\n",
    "        logits = self.classifier(features)     # Classification용 로짓 (원본 특징 기반)\n",
    "\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # EfficientNet 권장 입력 크기 유지\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 개선된 모델 초기화\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "\n",
    "    # [핵심 변경 3] Label Smoothing 제거 (기본값 0.0 사용)\n",
    "    # 모델이 정답에 더 강한 확신을 갖도록 유도\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with Improved EfficientNet-B4 for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # 수정된 forward 반환값 받기\n",
    "            embs, logits = model(imgs)\n",
    "\n",
    "            # 1. Classification Loss (Label Smoothing 제거됨)\n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            # weak 데이터에 대한 신뢰도 가중치는 적용\n",
    "            loss_c = (loss_c * confs).mean()\n",
    "\n",
    "            # 2. Contrastive Loss (Projection Head 출력 사용)\n",
    "            half = imgs.size(0) // 2\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "\n",
    "            # Loss 합산 (비율 유지)\n",
    "            total_loss = loss_c + 0.1 * loss_ct\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=total_loss.item(), acc=train_correct/train_total)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                # 수정된 forward 반환값 받기\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_cls(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        # 시각화 저장 (Temperature 3.0 유지 - 학습이 잘되면 분포가 넓어질 것임)\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919225e",
   "metadata": {},
   "source": [
    "mobile net 피쳐맵, 투영층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3992b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 9535 images.\n",
      "[Dataset] Loaded 1149 images.\n",
      "[INFO] Loading MobileNetV3-Large...\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /home/user5/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 84.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start Training (MobileNetV3 + Mixup) for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:37<00:00,  7.89it/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8009 | Val Acc: 0.9460 | Val F1: 0.9331\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:32<00:00,  9.06it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8500 | Val Acc: 0.9678 | Val F1: 0.9576\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:33<00:00,  8.75it/s, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8411 | Val Acc: 0.9721 | Val F1: 0.9636\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:33<00:00,  8.98it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8479 | Val Acc: 0.9521 | Val F1: 0.9403\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:33<00:00,  8.85it/s, loss=0.512] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8538 | Val Acc: 0.9756 | Val F1: 0.9684\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:30<00:00,  9.61it/s, loss=0.502] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8559 | Val Acc: 0.9513 | Val F1: 0.9392\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:32<00:00,  9.08it/s, loss=0.403] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8508 | Val Acc: 0.9565 | Val F1: 0.9454\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:33<00:00,  8.89it/s, loss=0.372] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8571 | Val Acc: 0.9652 | Val F1: 0.9558\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:33<00:00,  8.84it/s, loss=0.485] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8580 | Val Acc: 0.9678 | Val F1: 0.9591\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:31<00:00,  9.56it/s, loss=0.42]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8602 | Val Acc: 0.9765 | Val F1: 0.9696\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:36<00:00,  8.24it/s, loss=0.488] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8618 | Val Acc: 0.9756 | Val F1: 0.9686\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 297/297 [00:32<00:00,  9.08it/s, loss=0.328] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8694 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 297/297 [00:33<00:00,  8.89it/s, loss=0.569] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8597 | Val Acc: 0.9774 | Val F1: 0.9708\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_013.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 297/297 [00:35<00:00,  8.28it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8604 | Val Acc: 0.9617 | Val F1: 0.9517\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 297/297 [00:32<00:00,  9.14it/s, loss=0.37]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8547 | Val Acc: 0.9591 | Val F1: 0.9486\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 297/297 [00:33<00:00,  8.91it/s, loss=0.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8688 | Val Acc: 0.9695 | Val F1: 0.9611\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 297/297 [00:36<00:00,  8.05it/s, loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8680 | Val Acc: 0.9713 | Val F1: 0.9633\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_017.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 297/297 [00:32<00:00,  9.20it/s, loss=0.361] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8701 | Val Acc: 0.9678 | Val F1: 0.9590\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_018.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 297/297 [00:34<00:00,  8.72it/s, loss=0.569] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8606 | Val Acc: 0.9739 | Val F1: 0.9665\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 297/297 [00:36<00:00,  8.22it/s, loss=0.418] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8601 | Val Acc: 0.9782 | Val F1: 0.9719\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_020.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 297/297 [00:31<00:00,  9.37it/s, loss=0.386] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8558 | Val Acc: 0.9756 | Val F1: 0.9686\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_021.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 297/297 [00:32<00:00,  9.22it/s, loss=0.338] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8648 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_022.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30: 100%|██████████| 297/297 [00:31<00:00,  9.30it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8651 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_023.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/30: 100%|██████████| 297/297 [00:33<00:00,  8.78it/s, loss=0.394] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8582 | Val Acc: 0.9713 | Val F1: 0.9633\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_024.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/30: 100%|██████████| 297/297 [00:34<00:00,  8.62it/s, loss=0.382] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8739 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_025.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/30: 100%|██████████| 297/297 [00:33<00:00,  8.86it/s, loss=0.363] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8566 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_026.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27/30: 100%|██████████| 297/297 [00:34<00:00,  8.65it/s, loss=0.421] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8621 | Val Acc: 0.9721 | Val F1: 0.9644\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_027.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28/30: 100%|██████████| 297/297 [00:33<00:00,  8.78it/s, loss=0.403] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8693 | Val Acc: 0.9721 | Val F1: 0.9644\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_028.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29/30: 100%|██████████| 297/297 [00:32<00:00,  9.08it/s, loss=0.283] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8610 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_029.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30/30: 100%|██████████| 297/297 [00:32<00:00,  9.06it/s, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Train Acc: 0.8698 | Val Acc: 0.9730 | Val F1: 0.9654\n",
      "[INFO] Saved Preview: /workspace/user5/runs_mobilenet_mixup/val_previews_mixup/val_preview_epoch_030.png\n",
      "[Done] Model Saved: /workspace/user5/runs_mobilenet_mixup/emotion_model_mobilenet.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_mobilenet_mixup\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_mobilenet.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_mobilenet.csv\"\n",
    "\n",
    "# 이미지 저장 경로\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_mixup\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# [중요] Mixup 설정 (alpha가 클수록 더 많이 섞임. 0.1~1.0 사이 분포 유도)\n",
    "MIXUP_ALPHA = 1.0 \n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) Mixup 함수 (핵심 솔루션)\n",
    "# =========================\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# =========================\n",
    "# 2) 유틸리티: 로거\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss\", \"train/acc\",\n",
    "            \"val/loss\", \"val/acc\",\n",
    "            \"metrics/f1\", \"lr\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss'], label='Train Loss', marker='.')\n",
    "        plt.plot(df['epoch'], df['val/loss'], label='Val Loss', marker='.')\n",
    "        plt.title(\"Loss (Mixup Applied)\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# 3) 시각화 함수 (분포 확인용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(imgs)\n",
    "        # Mixup을 쓰면 Temperature Scaling 없이도 분포가 넓어지므로 기본값 사용\n",
    "        probs = F.softmax(logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "    \n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray): axes = axes.flatten()\n",
    "    else: axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "            \n",
    "            if pred_idx == 0: box_color = 'blue'\n",
    "            else: box_color = 'cyan'\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "            \n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "        ax.axis('off') \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved Preview: {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 4) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    conf = 1.0 # Mixup이 값을 조절하므로 여기선 1.0으로 둠\n",
    "                    self.items.append((img, class_id, conf))\n",
    "        print(f\"[Dataset] Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 5) Model: MobileNetV3-Large (Custom Head)\n",
    "# =========================\n",
    "class MobileNetEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetEmotionModel, self).__init__()\n",
    "        \n",
    "        print(\"[INFO] Loading MobileNetV3-Large...\")\n",
    "        # MobileNetV3 로드\n",
    "        self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # 마지막 분류기 교체\n",
    "        in_features = self.backbone.classifier[0].in_features\n",
    "        \n",
    "        # [핵심] Dropout을 강화하여 불확실성 유도\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 1280),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.4), # Dropout 비율 0.2 -> 0.4로 증가 (불확실성 유도)\n",
    "            nn.Linear(1280, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# =========================\n",
    "# 6) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3), # 강한 증강\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = MobileNetEmotionModel(num_classes=2).to(device)\n",
    "    \n",
    "    # Mixup을 사용하므로 단순 CrossEntropyLoss 사용 (Label Smoothing 불필요)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    print(f\"[INFO] Start Training (MobileNetV3 + Mixup) for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train() \n",
    "        r_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, _ in loop:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            \n",
    "            # [핵심] Mixup 적용\n",
    "            # inputs: 섞인 이미지\n",
    "            # targets_a, targets_b: 섞이기 전의 두 라벨\n",
    "            # lam: 섞인 비율 (예: 0.6)\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(imgs, lbls, MIXUP_ALPHA, use_cuda=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Mixup Loss 계산\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += loss.item()\n",
    "            \n",
    "            # 정확도 계산 (더 큰 비중을 가진 라벨을 정답으로 간주)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_total += lbls.size(0)\n",
    "            # Mixup 상황에서의 정확도는 근사치입니다.\n",
    "            train_correct += (lam * preds.eq(targets_a.data).cpu().sum().float()\n",
    "                            + (1 - lam) * preds.eq(targets_b.data).cpu().sum().float())\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation (No Mixup)\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss_v = criterion(outputs, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"   -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0daef895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Loaded 5245 happy images (conf 0.5~1.0).\n",
      "[Dataset] Loaded 1394 anger images (conf 1.0).\n",
      "[Dataset] Loaded 2896 other images (panic/sadness).\n",
      "[Dataset] Total Loaded 9535 images.\n",
      "[Dataset] Loaded 299 happy images (conf 0.5~1.0).\n",
      "[Dataset] Loaded 290 anger images (conf 1.0).\n",
      "[Dataset] Loaded 560 other images (panic/sadness).\n",
      "[Dataset] Total Loaded 1149 images.\n",
      "[INFO] Loading EfficientNet-B4 (Improved Structure)...\n",
      "[INFO] EfficientNet-B4 Feature Size: 1792\n",
      "[INFO] Start Training with Improved EfficientNet-B4 for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:59<00:00,  5.03it/s, acc=0.869, loss=0.0599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.8687 | Val Acc: 0.9556 | Val F1: 0.9438\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:54<00:00,  5.43it/s, acc=0.958, loss=0.045]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9580 | Val Acc: 0.9582 | Val F1: 0.9472\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.972, loss=0.115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9719 | Val Acc: 0.9591 | Val F1: 0.9487\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.978, loss=0.0588] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9777 | Val Acc: 0.9634 | Val F1: 0.9539\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:54<00:00,  5.43it/s, acc=0.982, loss=0.0224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9823 | Val Acc: 0.9591 | Val F1: 0.9486\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.985, loss=0.0155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9846 | Val Acc: 0.9730 | Val F1: 0.9655\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.988, loss=0.00987] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9881 | Val Acc: 0.9678 | Val F1: 0.9591\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.987, loss=0.00409] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9874 | Val Acc: 0.9652 | Val F1: 0.9560\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:54<00:00,  5.45it/s, acc=0.99, loss=0.0337]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9899 | Val Acc: 0.9704 | Val F1: 0.9621\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.991, loss=0.0254]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9912 | Val Acc: 0.9687 | Val F1: 0.9603\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.993, loss=0.00394] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9931 | Val Acc: 0.9713 | Val F1: 0.9634\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_imp/val_previews_effb4_imp/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30:  12%|█▏        | 35/297 [00:07<00:54,  4.78it/s, acc=0.993, loss=0.0635]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 491\u001b[0m\n\u001b[1;32m    488\u001b[0m     plot_results(CSV_PATH, SAVE_DIR)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 431\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_c \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_ct\n\u001b[1;32m    430\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 431\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m r_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    434\u001b[0m r_cls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_c\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:124\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    123\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tomas/lib/python3.10/site-packages/torch/optim/adam.py:651\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[0;32m--> 651\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import json # JSON 파일 처리를 위한 라이브러리 추가\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_effb4_imp\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effb4_imp.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effb4_imp.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_effb4_imp\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\", \"train/loss_cls\", \"train/loss_cont\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (EfficientNet-B4 Improved)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_effb4_imp.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward 수정으로 반환값이 (embeddings, logits)가 됨\n",
    "        _, logits = model(imgs)\n",
    "        # Temperature Scaling은 그대로 유지 (학습이 잘 되면 이 값에서도 분포가 넓어질 것임)\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=1)\n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "\n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "\n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "\n",
    "            if pred_idx == 0: box_color = 'blue'\n",
    "            else: box_color = 'cyan'\n",
    "\n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        # [수정된 부분 1] anger와 happy 데이터를 섞어서 사용할 수 있도록 모든 데이터를 한 번에 로드\n",
    "        happy_items = []\n",
    "        anger_items = []\n",
    "        \n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            for emotion_name, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                \n",
    "                # 데이터 로드 방식 변경: happy와 anger를 분리하여 저장\n",
    "                if emotion_name == \"happy\":\n",
    "                    for img in imgs:\n",
    "                        # happy: ID 0, 신뢰도 0.5~1.0 (균등 분포)\n",
    "                        conf = np.random.uniform(0.5, 1.0)\n",
    "                        happy_items.append((img, class_id, conf))\n",
    "                elif emotion_name == \"anger\":\n",
    "                    for img in imgs:\n",
    "                        # anger: ID 1, 신뢰도 1.0 (기본표정/확실한 other)\n",
    "                        conf = 1.0\n",
    "                        anger_items.append((img, class_id, conf))\n",
    "                else: # panic, sadness 등 other의 다른 감정들\n",
    "                    for img in imgs:\n",
    "                        # other 감정: ID 1, 약한 데이터(weak)만 0.6, 나머지는 1.0\n",
    "                        conf = 0.6 if \"weak\" in img.name.lower() else 1.0\n",
    "                        self.items.append((img, class_id, conf))\n",
    "        \n",
    "        # [수정된 부분 2] happy와 anger 데이터를 섞어서 전체 데이터셋에 추가\n",
    "        # 데이터 불균형 방지를 위해, 여기서는 단순 병합\n",
    "        self.items.extend(happy_items)\n",
    "        self.items.extend(anger_items)\n",
    "        \n",
    "        # 필요하다면 여기서 happy_items와 anger_items의 비율을 조정하여 샘플링 후 self.items에 추가할 수 있음\n",
    "        # 예: self.items.extend(random.sample(happy_items, k=num_happy_samples))\n",
    "        \n",
    "        print(f\"[Dataset] Loaded {len(happy_items)} happy images (conf 0.5~1.0).\")\n",
    "        print(f\"[Dataset] Loaded {len(anger_items)} anger images (conf 1.0).\")\n",
    "        print(f\"[Dataset] Loaded {len(self.items) - len(happy_items) - len(anger_items)} other images (panic/sadness).\")\n",
    "        print(f\"[Dataset] Total Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            # label은 0(happy) 또는 1(other)로 유지\n",
    "            # conf는 Classification Loss에 가중치로 사용\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            # 에러 발생 시 첫 번째 항목을 반환하여 학습 중단을 방지\n",
    "            return self.__getitem__(0)\n",
    "\n",
    "# =========================\n",
    "# 3) Model: 구조 개선된 EfficientNet-B4\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "\n",
    "        print(\"[INFO] Loading EfficientNet-B4 (Improved Structure)...\")\n",
    "        weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b4(weights=weights)\n",
    "\n",
    "        # EfficientNet-B4의 최종 특징 맵 차원 (보통 1792)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity() # 기존 분류기 제거\n",
    "\n",
    "        print(f\"[INFO] EfficientNet-B4 Feature Size: {in_features}\")\n",
    "\n",
    "        # [구조 변경 1] Projection Head (Contrastive Loss용, 128차원 압축)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "\n",
    "        # [구조 변경 2] Classifier (분류용, 원본 특징 직접 사용)\n",
    "        # 입력 차원이 Projection 출력이 아닌 Backbone 출력(in_features)이 됨\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Backbone 특징 추출 (풍부한 정보)\n",
    "        features = self.backbone(x) # [Batch, in_features]\n",
    "\n",
    "        # 2. 두 갈래로 분기\n",
    "        embeddings = self.projection(features) # Contrastive용 임베딩\n",
    "        logits = self.classifier(features)     # Classification용 로짓 (원본 특징 기반)\n",
    "\n",
    "        return embeddings, logits\n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function\n",
    "# =========================\n",
    "class NormalizedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # EfficientNet 권장 입력 크기 유지\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 개선된 모델 초기화\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "\n",
    "    # [핵심 변경 3] Label Smoothing 제거 (기본값 0.0 사용)\n",
    "    # 모델이 정답에 더 강한 확신을 갖도록 유도\n",
    "    criterion_cls = nn.CrossEntropyLoss(reduction='none') # reduction='none'으로 변경\n",
    "    criterion_cont = NormalizedContrastiveLoss(margin=1.0)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with Improved EfficientNet-B4 for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss, r_cls, r_cont = 0.0, 0.0, 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            # confs: happy 0.5~1.0, anger 1.0, others 0.6 or 1.0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # 수정된 forward 반환값 받기\n",
    "            embs, logits = model(imgs)\n",
    "\n",
    "            # 1. Classification Loss\n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            # [핵심 변경 4] 데이터셋에서 부여된 신뢰도 가중치(confs) 적용 후 평균\n",
    "            # happy에 0.5~1.0, anger에 1.0 가중치가 적용됨\n",
    "            loss_c = (loss_c * confs).mean() \n",
    "\n",
    "            # 2. Contrastive Loss (Projection Head 출력 사용)\n",
    "            half = imgs.size(0) // 2\n",
    "            # 이진 분류 레이블을 사용하여 Positive/Negative 쌍 결정\n",
    "            loss_ct = criterion_cont(embs[:half], embs[half:], (lbls[:half]!=lbls[half:]).float())\n",
    "\n",
    "            # Loss 합산 (비율 유지)\n",
    "            total_loss = loss_c + 0.1 * loss_ct\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r_loss += total_loss.item()\n",
    "            r_cls += loss_c.item()\n",
    "            r_cont += loss_ct.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=total_loss.item(), acc=train_correct/train_total)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        # Validation Loss는 confs 없이 계산 (일반적인 CrossEntropyLoss)\n",
    "        criterion_val = nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader: # Val Loader의 confs는 사용하지 않음\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                # 수정된 forward 반환값 받기\n",
    "                _, logits = model(imgs)\n",
    "                loss_v = criterion_val(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"  -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        # 시각화 저장 (Temperature 3.0 유지 - 학습이 잘되면 분포가 넓어질 것임)\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98814bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Preparing Datasets...\n",
      "[Dataset] Total Loaded 9535 images.\n",
      "[Dataset] Total Loaded 1149 images.\n",
      "[INFO] Loading EfficientNet-B4 (Classification Only)...\n",
      "[INFO] EfficientNet-B4 Feature Size: 1792\n",
      "[INFO] Start Training with Classification Only EfficientNet-B4 for 30 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 297/297 [00:58<00:00,  5.09it/s, acc=0.804, loss=0.294] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.8040 | Val Acc: 0.7842 | Val F1: 0.7839\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 297/297 [00:54<00:00,  5.44it/s, acc=0.897, loss=0.136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.8970 | Val Acc: 0.8294 | Val F1: 0.8293\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.918, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9177 | Val Acc: 0.8460 | Val F1: 0.8459\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.931, loss=0.207] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9313 | Val Acc: 0.8599 | Val F1: 0.8598\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_004.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.945, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9445 | Val Acc: 0.8616 | Val F1: 0.8616\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.949, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9494 | Val Acc: 0.8703 | Val F1: 0.8703\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_006.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.957, loss=0.0801] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9574 | Val Acc: 0.8625 | Val F1: 0.8624\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_007.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.958, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9584 | Val Acc: 0.8581 | Val F1: 0.8579\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_008.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.967, loss=0.0271] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9672 | Val Acc: 0.8703 | Val F1: 0.8703\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_009.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.974, loss=0.0301] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9742 | Val Acc: 0.8625 | Val F1: 0.8625\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.975, loss=0.142]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9754 | Val Acc: 0.8686 | Val F1: 0.8686\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_011.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.978, loss=0.0428] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9779 | Val Acc: 0.8590 | Val F1: 0.8590\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.979, loss=0.0106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9785 | Val Acc: 0.8712 | Val F1: 0.8711\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_013.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.982, loss=0.0206]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9819 | Val Acc: 0.8686 | Val F1: 0.8684\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.983, loss=0.00366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9832 | Val Acc: 0.8712 | Val F1: 0.8712\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.986, loss=0.122]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9859 | Val Acc: 0.8660 | Val F1: 0.8660\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_016.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.987, loss=0.124]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9872 | Val Acc: 0.8677 | Val F1: 0.8677\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_017.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.989, loss=0.00456] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9894 | Val Acc: 0.8686 | Val F1: 0.8685\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_018.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.99, loss=0.0116]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9904 | Val Acc: 0.8755 | Val F1: 0.8755\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_019.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 297/297 [00:54<00:00,  5.40it/s, acc=0.989, loss=0.0123]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9890 | Val Acc: 0.8712 | Val F1: 0.8711\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_020.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, acc=0.99, loss=0.00112]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9897 | Val Acc: 0.8764 | Val F1: 0.8763\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_021.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.991, loss=0.00781] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9905 | Val Acc: 0.8712 | Val F1: 0.8711\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_022.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.992, loss=0.00457] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9922 | Val Acc: 0.8668 | Val F1: 0.8666\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_023.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.991, loss=0.0192]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9907 | Val Acc: 0.8703 | Val F1: 0.8702\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_024.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/30: 100%|██████████| 297/297 [00:54<00:00,  5.41it/s, acc=0.994, loss=0.00523] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9938 | Val Acc: 0.8747 | Val F1: 0.8747\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_025.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/30: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, acc=0.992, loss=8.32e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9918 | Val Acc: 0.8764 | Val F1: 0.8763\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_026.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27/30: 100%|██████████| 297/297 [00:54<00:00,  5.43it/s, acc=0.993, loss=0.00575] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9928 | Val Acc: 0.8738 | Val F1: 0.8737\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_027.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.992, loss=0.00494] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9923 | Val Acc: 0.8773 | Val F1: 0.8773\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_028.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29/30: 100%|██████████| 297/297 [00:54<00:00,  5.42it/s, acc=0.993, loss=0.00614] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9933 | Val Acc: 0.8703 | Val F1: 0.8703\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_029.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30/30: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, acc=0.992, loss=0.214]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Train Acc: 0.9922 | Val Acc: 0.8729 | Val F1: 0.8729\n",
      "[INFO] Validation preview saved (T=3.0): /workspace/user5/runs_effb4_cls/val_previews_effb4_cls/val_preview_epoch_030.png\n",
      "[Done] Model Saved: /workspace/user5/runs_effb4_cls/emotion_model_effb4_cls.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 관련 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "# [수정된 부분]: Contrastive Loss 제거에 맞춰 경로 이름 변경\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_effb4_cls\") \n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effb4_cls.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effb4_cls.csv\"\n",
    "\n",
    "# 이미지 저장 경로 (시각화 결과)\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_effb4_cls\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로 (분류 레이블 재정의)\n",
    "# [핵심 수정 1] happy: {happy, anger} / other: {sadness, panic}\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": { # ID 0\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        },\n",
    "        \"anger\": { # Anger를 Happy 클래스로 이동\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        },\n",
    "    },\n",
    "    \"other\": { # ID 1\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": { # ID 0\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        },\n",
    "        \"anger\": { # Anger를 Happy 클래스로 이동\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        },\n",
    "    },\n",
    "    \"other\": { # ID 1\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "CLASS_TO_ID = {\"happy\": 0, \"other\": 1}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        # [수정된 부분]: Contrastive Loss 관련 항목 제거\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss_total\", \"train/acc\",\n",
    "            \"val/loss_total\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"lr/pg0\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss_total'], label='Train Loss', marker='.', color='blue')\n",
    "        plt.plot(df['epoch'], df['val/loss_total'], label='Val Loss', marker='.', color='red')\n",
    "        plt.title(\"Loss Curve (EfficientNet-B4 Classification Only)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', marker='.', color='blue', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc', marker='.', color='green')\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 3. Metrics\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/precision'], label='Precision', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/recall'], label='Recall', marker='.', alpha=0.7)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='F1 Score', marker='.', color='orange', linewidth=2)\n",
    "        plt.title(\"Validation Metrics\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # 4. LR\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df['epoch'], df['lr/pg0'], label='LR', color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_effb4_cls.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# [핵심 기능] 시각화 (Temperature Scaling 적용)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=3.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        # [수정된 부분]: next(data_iter)를 통해 이미지만 가져오도록 수정\n",
    "        imgs, _, _ = next(data_iter) \n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # [수정된 부분]: 모델 출력은 오직 logits\n",
    "        logits = model(imgs)\n",
    "        # Temperature Scaling은 그대로 유지\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=1)\n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "\n",
    "    imgs_unnorm = imgs * std + mean\n",
    "    imgs_unnorm = torch.clamp(imgs_unnorm, 0, 1)\n",
    "\n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            score = confidences[i].item()\n",
    "            pred_class_name = ID_TO_CLASS[pred_idx]\n",
    "\n",
    "            if pred_idx == 0: box_color = 'blue'\n",
    "            else: box_color = 'cyan'\n",
    "\n",
    "            ax.imshow(img_np)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "            label_text = f\"{pred_class_name} {score:.2f}\"\n",
    "            ax.text(5, 20, label_text, color='white', fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.8, edgecolor='none', pad=2))\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"val_preview_epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Validation preview saved (T={temperature}): {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for label_name, sub_sources in sources.items():\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            \n",
    "            for emotion_name, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                \n",
    "                for img in imgs:\n",
    "                    conf = 1.0 # 기본 신뢰도 1.0\n",
    "                    \n",
    "                    if emotion_name == \"happy\":\n",
    "                        # [핵심 수정 2-1]: happy는 신뢰도 1.0으로 강하게 학습\n",
    "                        conf = 1.0\n",
    "                    elif emotion_name == \"anger\":\n",
    "                        # [핵심 수정 2-2]: anger는 happy 클래스(ID 0)에 포함, 신뢰도 0.7 부여 (Soft Labeling 효과)\n",
    "                        # ID는 TRAIN_SOURCES에서 이미 ID 0으로 지정됨\n",
    "                        conf = 0.7 \n",
    "                    elif \"weak\" in img.name.lower():\n",
    "                        # sadness, panic 중 weak 데이터는 0.6\n",
    "                        conf = 0.6\n",
    "                        \n",
    "                    self.items.append((img, class_id, conf))\n",
    "                    \n",
    "        print(f\"[Dataset] Total Loaded {len(self.items)} images.\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Cannot load image {img_path}: {e}\")\n",
    "            # 에러 발생 시 데이터셋의 첫 번째 항목을 반환하여 학습 중단 방지\n",
    "            return self.__getitem__(0) \n",
    "\n",
    "# =========================\n",
    "# 3) Model: 순수 Classification Model\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "\n",
    "        print(\"[INFO] Loading EfficientNet-B4 (Classification Only)...\")\n",
    "        weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b4(weights=weights)\n",
    "\n",
    "        # EfficientNet-B4의 최종 특징 맵 차원 (1792)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        # [핵심 수정 3]: 기존 분류기 제거 후 새로운 분류기만 사용 (Projection Head 제거)\n",
    "        self.backbone.classifier = nn.Identity() \n",
    "\n",
    "        print(f\"[INFO] EfficientNet-B4 Feature Size: {in_features}\")\n",
    "        \n",
    "        # Classifier (분류용)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2), # 드롭아웃 추가\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    # [핵심 수정 3]: Contrastive Loss용 Embedding 반환 로직 제거\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x) # [Batch, in_features]\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # 이제 forward는 오직 logits만 반환\n",
    "        return logits \n",
    "\n",
    "# =========================\n",
    "# 4) Loss Function (Contrastive Loss 제거)\n",
    "# =========================\n",
    "# Contrastive Loss 클래스 제거\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # EfficientNet 권장 입력 크기 유지\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(256, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"[INFO] Preparing Datasets...\")\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 개선된 모델 초기화 (Classification Only)\n",
    "    model = RobustEmotionModel(num_classes=2).to(device)\n",
    "\n",
    "    # Classification Loss (reduction='none' 유지하여 가중치 적용)\n",
    "    criterion_cls = nn.CrossEntropyLoss(reduction='none') \n",
    "    \n",
    "    # Contrastive Loss 제거\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training with Classification Only EfficientNet-B4 for {NUM_EPOCHS} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            # confs: happy 1.0, anger 0.7 (ID 0), others 0.6 or 1.0 (ID 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # [수정된 부분]: forward는 오직 logits만 반환\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # 1. Classification Loss (Confs 가중치 적용)\n",
    "            loss_c = criterion_cls(logits, lbls)\n",
    "            loss_c = (loss_c * confs).mean() \n",
    "\n",
    "            # [수정된 부분]: Total Loss는 Classification Loss만 사용\n",
    "            total_loss = loss_c\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r_loss += total_loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == lbls).sum().item()\n",
    "            train_total += lbls.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=total_loss.item(), acc=train_correct/train_total)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        # Validation Loss는 일반 CrossEntropyLoss (가중치 미사용)\n",
    "        criterion_val = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader: # Val Loader의 confs는 사용하지 않음\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                \n",
    "                # [수정된 부분]: forward는 오직 logits만 반환\n",
    "                logits = model(imgs)\n",
    "                \n",
    "                loss_v = criterion_val(logits, lbls)\n",
    "                val_loss_sum += loss_v.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss_total\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss_total\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            # Contrastive Loss 관련 항목은 빈 값으로 남김\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\", \"lr/pg0\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        print(f\"  -> Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f1:.4f}\")\n",
    "\n",
    "        # 시각화 저장\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=3.0)\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa1b9d",
   "metadata": {},
   "source": [
    "anger기본으로하고 happy분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050984d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running on cuda\n",
      "[Dataset] Total: 9535 | Counts: {0: 5245, 1: 1394, 2: 2896}\n",
      "[Dataset] Total: 1149 | Counts: {0: 299, 1: 290, 2: 560}\n",
      "[INFO] Building Model for 3 Classes...\n",
      "[INFO] Start Training (3 Classes, Focal Loss)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/100: 100%|██████████| 297/297 [00:56<00:00,  5.28it/s, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.7894 | Macro F1: 0.7660\n",
      " -> [Class F1] Happy: 0.8830, Avg: 0.5959, Other: 0.8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8024 | Macro F1: 0.7768\n",
      " -> [Class F1] Happy: 0.8879, Avg: 0.6102, Other: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8268 | Macro F1: 0.8111\n",
      " -> [Class F1] Happy: 0.9077, Avg: 0.6771, Other: 0.8484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/100: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, loss=0.0776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8390 | Macro F1: 0.8248\n",
      " -> [Class F1] Happy: 0.9219, Avg: 0.6955, Other: 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8442 | Macro F1: 0.8316\n",
      " -> [Class F1] Happy: 0.9233, Avg: 0.7107, Other: 0.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.047] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8590 | Macro F1: 0.8487\n",
      " -> [Class F1] Happy: 0.9280, Avg: 0.7447, Other: 0.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.055] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8573 | Macro F1: 0.8464\n",
      " -> [Class F1] Happy: 0.9206, Avg: 0.7438, Other: 0.8747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8564 | Macro F1: 0.8475\n",
      " -> [Class F1] Happy: 0.9221, Avg: 0.7491, Other: 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8538 | Macro F1: 0.8434\n",
      " -> [Class F1] Happy: 0.9122, Avg: 0.7467, Other: 0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8590 | Macro F1: 0.8494\n",
      " -> [Class F1] Happy: 0.9177, Avg: 0.7551, Other: 0.8753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/100: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, loss=0.0747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8590 | Macro F1: 0.8502\n",
      " -> [Class F1] Happy: 0.9320, Avg: 0.7467, Other: 0.8718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8433 | Macro F1: 0.8292\n",
      " -> [Class F1] Happy: 0.9017, Avg: 0.7202, Other: 0.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8555 | Macro F1: 0.8426\n",
      " -> [Class F1] Happy: 0.9146, Avg: 0.7387, Other: 0.8747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8520 | Macro F1: 0.8434\n",
      " -> [Class F1] Happy: 0.9260, Avg: 0.7376, Other: 0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/100: 100%|██████████| 297/297 [00:55<00:00,  5.38it/s, loss=0.0517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8494 | Macro F1: 0.8408\n",
      " -> [Class F1] Happy: 0.9080, Avg: 0.7500, Other: 0.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8581 | Macro F1: 0.8500\n",
      " -> [Class F1] Happy: 0.9165, Avg: 0.7595, Other: 0.8739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8564 | Macro F1: 0.8474\n",
      " -> [Class F1] Happy: 0.9236, Avg: 0.7495, Other: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/100: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, loss=0.0346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8616 | Macro F1: 0.8518\n",
      " -> [Class F1] Happy: 0.9189, Avg: 0.7590, Other: 0.8774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8564 | Macro F1: 0.8517\n",
      " -> [Class F1] Happy: 0.9216, Avg: 0.7668, Other: 0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8599 | Macro F1: 0.8537\n",
      " -> [Class F1] Happy: 0.9233, Avg: 0.7681, Other: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/100: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, loss=0.0268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8503 | Macro F1: 0.8402\n",
      " -> [Class F1] Happy: 0.9051, Avg: 0.7481, Other: 0.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8564 | Macro F1: 0.8461\n",
      " -> [Class F1] Happy: 0.9130, Avg: 0.7524, Other: 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8660 | Macro F1: 0.8568\n",
      " -> [Class F1] Happy: 0.9255, Avg: 0.7646, Other: 0.8803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8677 | Macro F1: 0.8592\n",
      " -> [Class F1] Happy: 0.9221, Avg: 0.7726, Other: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/100: 100%|██████████| 297/297 [00:55<00:00,  5.37it/s, loss=0.0278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8581 | Macro F1: 0.8495\n",
      " -> [Class F1] Happy: 0.9125, Avg: 0.7624, Other: 0.8735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8581 | Macro F1: 0.8486\n",
      " -> [Class F1] Happy: 0.9206, Avg: 0.7519, Other: 0.8732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27/100: 100%|██████████| 297/297 [00:56<00:00,  5.27it/s, loss=0.0378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8660 | Macro F1: 0.8591\n",
      " -> [Class F1] Happy: 0.9209, Avg: 0.7788, Other: 0.8776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28/100: 100%|██████████| 297/297 [00:55<00:00,  5.31it/s, loss=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8616 | Macro F1: 0.8532\n",
      " -> [Class F1] Happy: 0.9211, Avg: 0.7630, Other: 0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8625 | Macro F1: 0.8538\n",
      " -> [Class F1] Happy: 0.9233, Avg: 0.7612, Other: 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8581 | Macro F1: 0.8488\n",
      " -> [Class F1] Happy: 0.9134, Avg: 0.7589, Other: 0.8741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 31/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8607 | Macro F1: 0.8516\n",
      " -> [Class F1] Happy: 0.9387, Avg: 0.7447, Other: 0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 32/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8642 | Macro F1: 0.8576\n",
      " -> [Class F1] Happy: 0.9142, Avg: 0.7807, Other: 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 33/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8703 | Macro F1: 0.8585\n",
      " -> [Class F1] Happy: 0.9299, Avg: 0.7589, Other: 0.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 34/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8721 | Macro F1: 0.8655\n",
      " -> [Class F1] Happy: 0.9231, Avg: 0.7898, Other: 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 35/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8660 | Macro F1: 0.8559\n",
      " -> [Class F1] Happy: 0.9268, Avg: 0.7595, Other: 0.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 36/100: 100%|██████████| 297/297 [00:55<00:00,  5.31it/s, loss=0.0239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8634 | Macro F1: 0.8518\n",
      " -> [Class F1] Happy: 0.9211, Avg: 0.7534, Other: 0.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 37/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8634 | Macro F1: 0.8556\n",
      " -> [Class F1] Happy: 0.9177, Avg: 0.7709, Other: 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 38/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8668 | Macro F1: 0.8616\n",
      " -> [Class F1] Happy: 0.9243, Avg: 0.7851, Other: 0.8756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 39/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8651 | Macro F1: 0.8574\n",
      " -> [Class F1] Happy: 0.9314, Avg: 0.7644, Other: 0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 40/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8590 | Macro F1: 0.8531\n",
      " -> [Class F1] Happy: 0.9231, Avg: 0.7673, Other: 0.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 41/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8799 | Macro F1: 0.8735\n",
      " -> [Class F1] Happy: 0.9453, Avg: 0.7880, Other: 0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 42/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8634 | Macro F1: 0.8559\n",
      " -> [Class F1] Happy: 0.9211, Avg: 0.7690, Other: 0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 43/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8642 | Macro F1: 0.8554\n",
      " -> [Class F1] Happy: 0.9272, Avg: 0.7612, Other: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 44/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8651 | Macro F1: 0.8589\n",
      " -> [Class F1] Happy: 0.9314, Avg: 0.7726, Other: 0.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 45/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8773 | Macro F1: 0.8713\n",
      " -> [Class F1] Happy: 0.9479, Avg: 0.7802, Other: 0.8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 46/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8676\n",
      " -> [Class F1] Happy: 0.9391, Avg: 0.7804, Other: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 47/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8684\n",
      " -> [Class F1] Happy: 0.9452, Avg: 0.7784, Other: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 48/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8695 | Macro F1: 0.8625\n",
      " -> [Class F1] Happy: 0.9348, Avg: 0.7741, Other: 0.8787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 49/100: 100%|██████████| 297/297 [00:56<00:00,  5.23it/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8729 | Macro F1: 0.8670\n",
      " -> [Class F1] Happy: 0.9408, Avg: 0.7784, Other: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 50/100: 100%|██████████| 297/297 [00:56<00:00,  5.29it/s, loss=0.021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8729 | Macro F1: 0.8651\n",
      " -> [Class F1] Happy: 0.9378, Avg: 0.7732, Other: 0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 51/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8677 | Macro F1: 0.8592\n",
      " -> [Class F1] Happy: 0.9376, Avg: 0.7603, Other: 0.8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 52/100: 100%|██████████| 297/297 [00:55<00:00,  5.39it/s, loss=0.022] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8695 | Macro F1: 0.8639\n",
      " -> [Class F1] Happy: 0.9342, Avg: 0.7798, Other: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 53/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8695 | Macro F1: 0.8591\n",
      " -> [Class F1] Happy: 0.9304, Avg: 0.7630, Other: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 54/100: 100%|██████████| 297/297 [00:55<00:00,  5.36it/s, loss=0.0257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8721 | Macro F1: 0.8649\n",
      " -> [Class F1] Happy: 0.9435, Avg: 0.7695, Other: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 55/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8816 | Macro F1: 0.8769\n",
      " -> [Class F1] Happy: 0.9398, Avg: 0.8014, Other: 0.8893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 56/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8686 | Macro F1: 0.8586\n",
      " -> [Class F1] Happy: 0.9287, Avg: 0.7639, Other: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 57/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8680\n",
      " -> [Class F1] Happy: 0.9312, Avg: 0.7892, Other: 0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 58/100: 100%|██████████| 297/297 [00:55<00:00,  5.35it/s, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8651 | Macro F1: 0.8570\n",
      " -> [Class F1] Happy: 0.9342, Avg: 0.7587, Other: 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 59/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8721 | Macro F1: 0.8618\n",
      " -> [Class F1] Happy: 0.9342, Avg: 0.7650, Other: 0.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 60/100: 100%|██████████| 297/297 [00:56<00:00,  5.29it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8655\n",
      " -> [Class F1] Happy: 0.9319, Avg: 0.7782, Other: 0.8863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 61/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8703 | Macro F1: 0.8618\n",
      " -> [Class F1] Happy: 0.9340, Avg: 0.7678, Other: 0.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 62/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8686 | Macro F1: 0.8604\n",
      " -> [Class F1] Happy: 0.9355, Avg: 0.7649, Other: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 63/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8680\n",
      " -> [Class F1] Happy: 0.9433, Avg: 0.7782, Other: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 64/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8686\n",
      " -> [Class F1] Happy: 0.9393, Avg: 0.7849, Other: 0.8815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 65/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8712 | Macro F1: 0.8626\n",
      " -> [Class F1] Happy: 0.9348, Avg: 0.7698, Other: 0.8832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 66/100: 100%|██████████| 297/297 [00:56<00:00,  5.29it/s, loss=0.0219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8695 | Macro F1: 0.8614\n",
      " -> [Class F1] Happy: 0.9346, Avg: 0.7687, Other: 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 67/100: 100%|██████████| 297/297 [00:56<00:00,  5.24it/s, loss=0.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8712 | Macro F1: 0.8651\n",
      " -> [Class F1] Happy: 0.9400, Avg: 0.7745, Other: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 68/100: 100%|██████████| 297/297 [00:55<00:00,  5.31it/s, loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8653\n",
      " -> [Class F1] Happy: 0.9363, Avg: 0.7730, Other: 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 69/100: 100%|██████████| 297/297 [00:56<00:00,  5.25it/s, loss=0.0203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8764 | Macro F1: 0.8690\n",
      " -> [Class F1] Happy: 0.9340, Avg: 0.7850, Other: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 70/100: 100%|██████████| 297/297 [00:55<00:00,  5.31it/s, loss=0.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8721 | Macro F1: 0.8648\n",
      " -> [Class F1] Happy: 0.9402, Avg: 0.7718, Other: 0.8825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 71/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8677 | Macro F1: 0.8600\n",
      " -> [Class F1] Happy: 0.9265, Avg: 0.7757, Other: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 72/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8773 | Macro F1: 0.8722\n",
      " -> [Class F1] Happy: 0.9450, Avg: 0.7877, Other: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 73/100: 100%|██████████| 297/297 [00:56<00:00,  5.30it/s, loss=0.022] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8877 | Macro F1: 0.8841\n",
      " -> [Class F1] Happy: 0.9498, Avg: 0.8092, Other: 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 74/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8860 | Macro F1: 0.8788\n",
      " -> [Class F1] Happy: 0.9498, Avg: 0.7917, Other: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 75/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8782 | Macro F1: 0.8723\n",
      " -> [Class F1] Happy: 0.9436, Avg: 0.7856, Other: 0.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 76/100: 100%|██████████| 297/297 [00:56<00:00,  5.27it/s, loss=0.023] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8825 | Macro F1: 0.8772\n",
      " -> [Class F1] Happy: 0.9417, Avg: 0.7993, Other: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 77/100: 100%|██████████| 297/297 [00:56<00:00,  5.25it/s, loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8816 | Macro F1: 0.8758\n",
      " -> [Class F1] Happy: 0.9429, Avg: 0.7941, Other: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 78/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8808 | Macro F1: 0.8732\n",
      " -> [Class F1] Happy: 0.9372, Avg: 0.7909, Other: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 79/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8634 | Macro F1: 0.8497\n",
      " -> [Class F1] Happy: 0.9116, Avg: 0.7535, Other: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 80/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8755 | Macro F1: 0.8685\n",
      " -> [Class F1] Happy: 0.9314, Avg: 0.7874, Other: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 81/100: 100%|██████████| 297/297 [00:57<00:00,  5.21it/s, loss=0.0229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8860 | Macro F1: 0.8815\n",
      " -> [Class F1] Happy: 0.9511, Avg: 0.8014, Other: 0.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 82/100: 100%|██████████| 297/297 [00:56<00:00,  5.30it/s, loss=0.0338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8851 | Macro F1: 0.8809\n",
      " -> [Class F1] Happy: 0.9475, Avg: 0.8036, Other: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 83/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8729 | Macro F1: 0.8688\n",
      " -> [Class F1] Happy: 0.9493, Avg: 0.7792, Other: 0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 84/100: 100%|██████████| 297/297 [00:56<00:00,  5.30it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8712 | Macro F1: 0.8625\n",
      " -> [Class F1] Happy: 0.9406, Avg: 0.7637, Other: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 85/100: 100%|██████████| 297/297 [00:55<00:00,  5.33it/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8834 | Macro F1: 0.8758\n",
      " -> [Class F1] Happy: 0.9404, Avg: 0.7932, Other: 0.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 86/100: 100%|██████████| 297/297 [00:56<00:00,  5.28it/s, loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8686 | Macro F1: 0.8628\n",
      " -> [Class F1] Happy: 0.9372, Avg: 0.7734, Other: 0.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 87/100: 100%|██████████| 297/297 [00:56<00:00,  5.22it/s, loss=0.0204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8825 | Macro F1: 0.8765\n",
      " -> [Class F1] Happy: 0.9408, Avg: 0.7978, Other: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 88/100: 100%|██████████| 297/297 [00:56<00:00,  5.26it/s, loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8764 | Macro F1: 0.8688\n",
      " -> [Class F1] Happy: 0.9316, Avg: 0.7865, Other: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 89/100: 100%|██████████| 297/297 [00:56<00:00,  5.28it/s, loss=0.0214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8782 | Macro F1: 0.8738\n",
      " -> [Class F1] Happy: 0.9365, Avg: 0.8000, Other: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 90/100: 100%|██████████| 297/297 [00:56<00:00,  5.28it/s, loss=0.0208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8799 | Macro F1: 0.8749\n",
      " -> [Class F1] Happy: 0.9408, Avg: 0.7957, Other: 0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 91/100: 100%|██████████| 297/297 [00:56<00:00,  5.22it/s, loss=0.0209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8664\n",
      " -> [Class F1] Happy: 0.9173, Avg: 0.7948, Other: 0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 92/100: 100%|██████████| 297/297 [00:58<00:00,  5.08it/s, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8773 | Macro F1: 0.8726\n",
      " -> [Class F1] Happy: 0.9277, Avg: 0.8042, Other: 0.8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 93/100: 100%|██████████| 297/297 [00:56<00:00,  5.26it/s, loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8782 | Macro F1: 0.8711\n",
      " -> [Class F1] Happy: 0.9417, Avg: 0.7836, Other: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 94/100: 100%|██████████| 297/297 [00:55<00:00,  5.31it/s, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8755 | Macro F1: 0.8674\n",
      " -> [Class F1] Happy: 0.9361, Avg: 0.7788, Other: 0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 95/100: 100%|██████████| 297/297 [00:56<00:00,  5.24it/s, loss=0.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8721 | Macro F1: 0.8656\n",
      " -> [Class F1] Happy: 0.9395, Avg: 0.7748, Other: 0.8825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 96/100: 100%|██████████| 297/297 [00:56<00:00,  5.30it/s, loss=0.0265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8790 | Macro F1: 0.8723\n",
      " -> [Class F1] Happy: 0.9419, Avg: 0.7853, Other: 0.8897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 97/100: 100%|██████████| 297/297 [00:55<00:00,  5.34it/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8738 | Macro F1: 0.8686\n",
      " -> [Class F1] Happy: 0.9433, Avg: 0.7824, Other: 0.8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 98/100: 100%|██████████| 297/297 [00:56<00:00,  5.28it/s, loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8877 | Macro F1: 0.8830\n",
      " -> [Class F1] Happy: 0.9482, Avg: 0.8065, Other: 0.8944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 99/100: 100%|██████████| 297/297 [00:55<00:00,  5.32it/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8755 | Macro F1: 0.8691\n",
      " -> [Class F1] Happy: 0.9327, Avg: 0.7890, Other: 0.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 100/100: 100%|██████████| 297/297 [00:56<00:00,  5.29it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.8773 | Macro F1: 0.8737\n",
      " -> [Class F1] Happy: 0.9391, Avg: 0.7979, Other: 0.8840\n",
      "[Done] Model Saved: /workspace/user5/runs_efficientnet_3class/emotion_model_effb4_3class.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 및 메트릭 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터 (Happy Tuning Focus)\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# [핵심] Happy 클래스 튜닝을 위한 가중치 설정\n",
    "# Happy 클래스의 중요도를 높이려면 첫 번째 값을 1.0보다 크게 설정 (예: 1.2)\n",
    "# 데이터가 적은 Average 클래스를 보정하려면 두 번째 값을 높임\n",
    "CLASS_WEIGHTS = [1.0, 1.0, 1.0] \n",
    "\n",
    "# [핵심] Label Smoothing: 0.1 (너무 강한 확신 방지 -> 일반화 성능 향상)\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_efficientnet_3class\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effb4_3class.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effb4_3class.csv\"\n",
    "\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews_effb4\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 소스 경로 (3-Class 구조로 변경)\n",
    "# =========================\n",
    "# 주의: 'anger' 데이터를 'average' 클래스로 이동시키고, 'other'에서는 제외하여 중복을 방지했습니다.\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {  # Class 0\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": { # Class 1 (Anger를 Average로 할당)\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {   # Class 2 (Anger 제외, Panic/Sadness만 유지)\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# 클래스 정의 업데이트\n",
    "CLASS_TO_ID = {\"happy\": 0, \"average\": 1, \"other\": 2}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"average\", 2: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸리티: 로거 & 결과 그래프\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\n",
    "            \"epoch\", \"time\",\n",
    "            \"train/loss\", \"train/acc\",\n",
    "            \"val/loss\", \"val/acc\",\n",
    "            \"metrics/precision\", \"metrics/recall\", \"metrics/f1\",\n",
    "            \"happy_f1\", \"avg_f1\", \"other_f1\", # 클래스별 세부 지표 추가\n",
    "            \"lr\"\n",
    "        ]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.header)\n",
    "\n",
    "    def log(self, data: dict):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([data.get(h, \"\") for h in self.header])\n",
    "\n",
    "def plot_results(csv_file, save_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # 1. Loss\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(df['epoch'], df['train/loss'], label='Train Loss', marker='.')\n",
    "        plt.plot(df['epoch'], df['val/loss'], label='Val Loss', marker='.')\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # 2. Accuracy\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(df['epoch'], df['train/acc'], label='Train Acc', linestyle='--')\n",
    "        plt.plot(df['epoch'], df['val/acc'], label='Val Acc')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # 3. Macro Metrics\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(df['epoch'], df['metrics/f1'], label='Macro F1', color='orange')\n",
    "        plt.title(\"Macro F1 Score\")\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # 4. Class-wise F1 (Happy vs others) - 중요!\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(df['epoch'], df['happy_f1'], label='Happy F1', color='blue', linewidth=2)\n",
    "        plt.plot(df['epoch'], df['avg_f1'], label='Avg F1', color='green', alpha=0.5)\n",
    "        plt.plot(df['epoch'], df['other_f1'], label='Other F1', color='red', alpha=0.5)\n",
    "        plt.title(\"Class-wise F1 Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # 5. LR\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.plot(df['epoch'], df['lr'], color='purple')\n",
    "        plt.title(\"Learning Rate\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / \"results_analysis.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Plotting failed: {e}\")\n",
    "\n",
    "# =========================\n",
    "# 시각화 (3-Class 대응)\n",
    "# =========================\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir, num_images=16, temperature=1.0):\n",
    "    model.eval()\n",
    "    try:\n",
    "        data_iter = iter(val_loader)\n",
    "        imgs, lbls, _ = next(data_iter)\n",
    "    except StopIteration:\n",
    "        return\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        # Temperature Scaling\n",
    "        scaled_logits = logits / temperature \n",
    "        probs = F.softmax(scaled_logits, dim=1) \n",
    "        confidences, preds = torch.max(probs, dim=1)\n",
    "    \n",
    "    # Un-normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    imgs_unnorm = torch.clamp(imgs * std + mean, 0, 1)\n",
    "    \n",
    "    # Plotting\n",
    "    cols = 4\n",
    "    rows = math.ceil(min(imgs.size(0), num_images) / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    \n",
    "    if isinstance(axes, np.ndarray): axes = axes.flatten()\n",
    "    else: axes = [axes]\n",
    "\n",
    "    color_map = {0: 'blue', 1: 'green', 2: 'red'} # Happy: Blue, Avg: Green, Other: Red\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        if i < imgs.size(0):\n",
    "            img_np = imgs_unnorm[i].cpu().permute(1, 2, 0).numpy()\n",
    "            pred_idx = preds[i].item()\n",
    "            true_idx = lbls[i].item()\n",
    "            score = confidences[i].item()\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            \n",
    "            # 예측값에 따른 테두리 색상\n",
    "            box_color = color_map.get(pred_idx, 'gray')\n",
    "            # 틀리면 붉은 점선 테두리\n",
    "            linestyle = '-' if pred_idx == true_idx else '--'\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor(box_color)\n",
    "                spine.set_linewidth(3)\n",
    "                spine.set_linestyle(linestyle)\n",
    "            \n",
    "            label_text = f\"P:{ID_TO_CLASS[pred_idx]}\\nT:{ID_TO_CLASS[true_idx]}\\n{score:.2f}\"\n",
    "            ax.text(5, 30, label_text, color='white', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(facecolor=box_color, alpha=0.7, edgecolor='none'))\n",
    "\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f\"val_preview_ep{epoch:03d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\"]\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files += list(p.rglob(ext))\n",
    "    return files\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform=None):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        self.class_counts = {0:0, 1:0, 2:0}\n",
    "\n",
    "        for label_name, sub_sources in sources.items():\n",
    "            if label_name not in CLASS_TO_ID: continue\n",
    "            class_id = CLASS_TO_ID[label_name]\n",
    "            \n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                for img in imgs:\n",
    "                    # weak 이미지는 신뢰도 낮춤 (0.1~0.9 튜닝의 일환)\n",
    "                    conf = 0.7 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf))\n",
    "                    self.class_counts[class_id] += 1\n",
    "                    \n",
    "        print(f\"[Dataset] Total: {len(self.items)} | Counts: {self.class_counts}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label, conf\n",
    "        except Exception:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# =========================\n",
    "# 3) Model & Custom Loss (Focal Loss)\n",
    "# =========================\n",
    "class RobustEmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(RobustEmotionModel, self).__init__()\n",
    "        print(f\"[INFO] Building Model for {num_classes} Classes...\")\n",
    "        weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b4(weights=weights)\n",
    "        \n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        # Projection Head (Contrastive Learning용)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.projection(features)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return embeddings, logits\n",
    "\n",
    "# [튜닝 핵심] Focal Loss: 0.1~0.9 사이의 애매한 샘플(Hard example)을 집중 학습\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha # Class Weights\n",
    "        self.gamma = gamma # Focusing Parameter\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=LABEL_SMOOTHING)\n",
    "        pt = torch.exp(-ce_loss) # 예측 확률\n",
    "        \n",
    "        # alpha 적용\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        elif self.reduction == 'sum': return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# =========================\n",
    "# 4) 학습 루프\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Running on {device}\")\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    # Augmentation\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((260, 260)), # EfficientNet-B4 input size optimization\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((260, 260)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform=train_tf)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform=val_tf)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 3-Class 모델\n",
    "    model = RobustEmotionModel(num_classes=3).to(device)\n",
    "    \n",
    "    # Class Weight Tensor 생성\n",
    "    class_weights_tensor = torch.tensor(CLASS_WEIGHTS).to(device)\n",
    "    \n",
    "    # Focal Loss 사용 (일반 CE보다 세밀한 튜닝 가능)\n",
    "    criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # WarmRestarts 스케줄러: 주기적으로 LR을 리셋하여 Local Minima 탈출\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    print(f\"[INFO] Start Training (3 Classes, Focal Loss)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss = 0.0\n",
    "        train_preds, train_trues = [], []\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, logits = model(imgs)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = criterion(logits, lbls)\n",
    "            \n",
    "            # 신뢰도가 낮은 이미지(weak)에 대해 Loss를 줄여줌 (Noise Robustness)\n",
    "            confs = confs.to(device)\n",
    "            loss = (loss * confs).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_trues.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        scheduler.step() # Epoch 단위 스케줄러 업데이트\n",
    "\n",
    "        # Calculate Train Accuracy\n",
    "        train_acc = (np.array(train_preds) == np.array(train_trues)).mean()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                val_loss_sum += criterion(logits, lbls).item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "        \n",
    "        # Macro Metrics\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Class-wise F1 Score (Happy 클래스의 성능을 별도로 추적)\n",
    "        f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "        # f1_per_class 배열 순서: [Happy, Average, Other]\n",
    "        happy_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0\n",
    "        avg_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0\n",
    "        other_f1 = f1_per_class[2] if len(f1_per_class) > 2 else 0\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1, \"time\": f\"{elapsed:.1f}\",\n",
    "            \"train/loss\": f\"{r_loss/len(train_loader):.4f}\", \"train/acc\": f\"{train_acc:.4f}\",\n",
    "            \"val/loss\": f\"{val_loss_sum/len(val_loader):.4f}\", \"val/acc\": f\"{val_acc:.4f}\",\n",
    "            \"metrics/precision\": f\"{precision:.4f}\", \"metrics/recall\": f\"{recall:.4f}\",\n",
    "            \"metrics/f1\": f\"{f1:.4f}\",\n",
    "            \"happy_f1\": f\"{happy_f1:.4f}\", \"avg_f1\": f\"{avg_f1:.4f}\", \"other_f1\": f\"{other_f1:.4f}\",\n",
    "            \"lr\": f\"{current_lr:.6f}\"\n",
    "        }\n",
    "        logger.log(log_data)\n",
    "        \n",
    "        print(f\" -> Val Acc: {val_acc:.4f} | Macro F1: {f1:.4f}\")\n",
    "        print(f\" -> [Class F1] Happy: {happy_f1:.4f}, Avg: {avg_f1:.4f}, Other: {other_f1:.4f}\")\n",
    "\n",
    "        # 시각화 저장 (Temperature를 2.0으로 주어 Softmax 분포를 완만하게 하여 확인)\n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR, temperature=2.0)\n",
    "\n",
    "    # 최종 모델 저장\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Model Saved: {MODEL_SAVE_PATH}\")\n",
    "    plot_results(CSV_PATH, SAVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf70158",
   "metadata": {},
   "source": [
    "crop안쓴거ResNet+CBAM happy:anger:other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "--- Loading Datasets ---\n",
      "[Dataset] Loading images...\n",
      "  -> Found 1493 images in /workspace/merge_data/data/img/train/happy (Class: happy)\n",
      "  -> Found 1394 images in /workspace/user4/cropped/train/anger (Class: average)\n",
      "  -> Found 1368 images in /workspace/user4/cropped/train/panic (Class: other)\n",
      "  -> Found 1528 images in /workspace/user4/cropped/train/sadness (Class: other)\n",
      "[Dataset] Total: 5783 | Distribution: [1493 1394 2896]\n",
      "[Dataset] Loading images...\n",
      "  -> Found 299 images in /workspace/user4/cropped/val/happy (Class: happy)\n",
      "  -> Found 290 images in /workspace/user4/cropped/val/anger (Class: average)\n",
      "  -> Found 280 images in /workspace/user4/cropped/val/panic (Class: other)\n",
      "  -> Found 280 images in /workspace/user4/cropped/val/sadness (Class: other)\n",
      "[Dataset] Total: 1149 | Distribution: [299 290 560]\n",
      "[INFO] Creating Balanced Sampler...\n",
      "[INFO] Building ResNet50 + CBAM Attention Model...\n",
      "--- Start Training for 30 Epochs (ResNet+CBAM+Focal) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 180/180 [00:42<00:00,  4.25it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.5901 | F1 Macro: 0.4331\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5262 | [Other F1]: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 180/180 [00:45<00:00,  3.99it/s, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6162 | F1 Macro: 0.4626\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6156 | [Other F1]: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 180/180 [00:46<00:00,  3.85it/s, loss=0.0963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6153 | F1 Macro: 0.4628\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6095 | [Other F1]: 0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 180/180 [00:43<00:00,  4.16it/s, loss=0.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6458 | F1 Macro: 0.4830\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6515 | [Other F1]: 0.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 180/180 [00:43<00:00,  4.17it/s, loss=0.0854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6388 | F1 Macro: 0.4778\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6519 | [Other F1]: 0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 180/180 [00:43<00:00,  4.12it/s, loss=0.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6406 | F1 Macro: 0.4775\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6565 | [Other F1]: 0.7759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 180/180 [00:45<00:00,  4.00it/s, loss=0.0685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6275 | F1 Macro: 0.4703\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6316 | [Other F1]: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 180/180 [00:46<00:00,  3.90it/s, loss=0.0428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6345 | F1 Macro: 0.4739\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6301 | [Other F1]: 0.7916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 180/180 [00:50<00:00,  3.60it/s, loss=0.0421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6353 | F1 Macro: 0.4743\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6284 | [Other F1]: 0.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 180/180 [00:53<00:00,  3.36it/s, loss=0.0335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6406 | F1 Macro: 0.4794\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6582 | [Other F1]: 0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 180/180 [00:52<00:00,  3.43it/s, loss=0.0469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6049 | F1 Macro: 0.4506\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5732 | [Other F1]: 0.7785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 180/180 [00:51<00:00,  3.48it/s, loss=0.0335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6301 | F1 Macro: 0.4743\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6612 | [Other F1]: 0.7617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 180/180 [00:52<00:00,  3.45it/s, loss=0.035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6205 | F1 Macro: 0.4688\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6631 | [Other F1]: 0.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 180/180 [00:51<00:00,  3.49it/s, loss=0.0582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6205 | F1 Macro: 0.4604\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6125 | [Other F1]: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 180/180 [00:51<00:00,  3.49it/s, loss=0.0331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6136 | F1 Macro: 0.4581\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5920 | [Other F1]: 0.7822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 180/180 [00:53<00:00,  3.38it/s, loss=0.0288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6057 | F1 Macro: 0.4572\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6309 | [Other F1]: 0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 180/180 [00:51<00:00,  3.48it/s, loss=0.0275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6144 | F1 Macro: 0.4592\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6176 | [Other F1]: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 180/180 [00:51<00:00,  3.50it/s, loss=0.035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6249 | F1 Macro: 0.4743\n",
      " -> [Happy F1]: 0.0198 | [Avg F1]: 0.6496 | [Other F1]: 0.7535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 180/180 [00:51<00:00,  3.48it/s, loss=0.0296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6292 | F1 Macro: 0.4702\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6374 | [Other F1]: 0.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 180/180 [00:51<00:00,  3.46it/s, loss=0.0254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6275 | F1 Macro: 0.4696\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6355 | [Other F1]: 0.7734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 180/180 [00:52<00:00,  3.45it/s, loss=0.0305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6440 | F1 Macro: 0.4865\n",
      " -> [Happy F1]: 0.0066 | [Avg F1]: 0.6921 | [Other F1]: 0.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 180/180 [00:54<00:00,  3.28it/s, loss=0.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6379 | F1 Macro: 0.4809\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6848 | [Other F1]: 0.7581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30:  46%|████▌     | 83/180 [00:24<00:28,  3.42it/s, loss=0.0411]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 434\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Done] Saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 385\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m _, logits \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m    384\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, lbls)\n\u001b[0;32m--> 385\u001b[0m confs \u001b[38;5;241m=\u001b[39m \u001b[43mconfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m loss \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m*\u001b[39m confs)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    388\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 시각화 및 데이터 처리\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# [중요] 손상된 이미지 로딩 허용\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_resnet_cbam_final\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_resnet_cbam_final.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_resnet_cbam_final.csv\"\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 클래스 정의\n",
    "CLASS_TO_ID = {\"happy\": 0, \"average\": 1, \"other\": 2}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"average\", 2: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 데이터 소스\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {  \n",
    "        \"happy\": {\n",
    "            # 원본(Raw) 이미지 -> Dataset 클래스에서 is_raw=True로 판별됨\n",
    "            \"img_dir\": \"/workspace/merge_data/data/img/train/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": { \n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {   \n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋 (수정됨: 로직 강화)\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    extensions = [\"*.jpg\", \"*.JPG\", \"*.jpeg\", \"*.JPEG\", \"*.png\", \"*.PNG\", \"*.bmp\", \"*.BMP\"]\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files += list(p.rglob(ext))\n",
    "    return sorted(list(set(files)))\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform_cropped=None, transform_raw=None):\n",
    "        self.items = []\n",
    "        self.transform_cropped = transform_cropped\n",
    "        self.transform_raw = transform_raw\n",
    "        self.labels = [] \n",
    "\n",
    "        print(\"[Dataset] Loading images...\")\n",
    "        for label_group, sub_sources in sources.items():\n",
    "            if label_group not in CLASS_TO_ID: continue\n",
    "            class_id = CLASS_TO_ID[label_group]\n",
    "            \n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                print(f\"  -> Found {len(imgs)} images in {info['img_dir']} (Class: {label_group})\")\n",
    "                \n",
    "                # [핵심 수정] 경로 문자열 대신 논리적으로 판단\n",
    "                # 1. 클래스가 'happy'이고\n",
    "                # 2. 경로에 'cropped'라는 단어가 포함되지 않았다면 -> 원본(Raw) 이미지\n",
    "                is_raw = False\n",
    "                if label_group == \"happy\" and \"cropped\" not in str(info[\"img_dir\"]):\n",
    "                    is_raw = True\n",
    "\n",
    "                for img in imgs:\n",
    "                    conf = 0.7 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf, is_raw))\n",
    "                    self.labels.append(class_id)\n",
    "        \n",
    "        self.class_counts = np.bincount(self.labels) if self.labels else np.array([0,0,0])\n",
    "        print(f\"[Dataset] Total: {len(self.items)} | Distribution: {self.class_counts}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf, is_raw = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # [전처리 분기]\n",
    "            # 1. Train Happy (Raw): transform_raw (Zoom-in) 적용\n",
    "            if is_raw and self.transform_raw:\n",
    "                image = self.transform_raw(image)\n",
    "            # 2. Train Others & Validation: transform_cropped (Standard) 적용\n",
    "            elif self.transform_cropped:\n",
    "                image = self.transform_cropped(image)\n",
    "                \n",
    "            return image, label, conf\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def get_weights(self):\n",
    "        count_arr = np.maximum(self.class_counts, 1) \n",
    "        class_weights = 1.0 / count_arr\n",
    "        sample_weights = [class_weights[label] for label in self.labels]\n",
    "        return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "# =========================\n",
    "# 3) 모델 & Loss 정의 (Focal Loss 추가)\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=0.1)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        return loss.sum()\n",
    "\n",
    "class ResNetCBAMEmotion(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ResNetCBAMEmotion, self).__init__()\n",
    "        # CBAM Components (Inner Classes)\n",
    "        class ChannelAttention(nn.Module):\n",
    "            def __init__(self, in_planes, ratio=16):\n",
    "                super(ChannelAttention, self).__init__()\n",
    "                self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "                self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "                self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "                self.relu1 = nn.ReLU()\n",
    "                self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "            def forward(self, x):\n",
    "                avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "                max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "                return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "        class SpatialAttention(nn.Module):\n",
    "            def __init__(self, kernel_size=7):\n",
    "                super(SpatialAttention, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "            def forward(self, x):\n",
    "                avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "                max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "                return self.sigmoid(self.conv1(torch.cat([avg_out, max_out], dim=1)))\n",
    "\n",
    "        class CBAMBlock(nn.Module):\n",
    "            def __init__(self, in_planes):\n",
    "                super(CBAMBlock, self).__init__()\n",
    "                self.ca = ChannelAttention(in_planes)\n",
    "                self.sa = SpatialAttention()\n",
    "            def forward(self, x):\n",
    "                x = x * self.ca(x)\n",
    "                x = x * self.sa(x)\n",
    "                return x\n",
    "\n",
    "        print(\"[INFO] Building ResNet50 + CBAM Attention Model...\")\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        self.backbone = models.resnet50(weights=weights)\n",
    "        \n",
    "        # ResNet의 각 Layer 끝에 CBAM을 달아주는 것이 좋지만, \n",
    "        # 구조를 단순화하여 Feature Extractor 끝단에 Attention 적용\n",
    "        self.feature_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Attention 적용을 위한 재구성 (B, 2048, 1, 1) 형태가 됨\n",
    "        # 하지만 여기선 단순화를 위해 Projection Head에서 처리\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) # (B, 2048)\n",
    "        emb = self.projection(x)\n",
    "        logits = self.classifier(emb)\n",
    "        return emb, logits\n",
    "\n",
    "# =========================\n",
    "# 4) 유틸리티 (Logger & Visualizer)\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"macro_f1\", \"happy_f1\", \"avg_f1\", \"other_f1\"]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            csv.writer(f).writerow(self.header)\n",
    "\n",
    "    def log(self, row):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir):\n",
    "    model.eval()\n",
    "    try:\n",
    "        imgs, lbls, _ = next(iter(val_loader))\n",
    "    except: return\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        _, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    imgs_vis = torch.clamp(imgs * std + mean, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(16, len(imgs))):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        img_np = imgs_vis[i].cpu().permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img_np)\n",
    "        pred_cls = ID_TO_CLASS[preds[i].item()]\n",
    "        true_cls = ID_TO_CLASS[lbls[i].item()]\n",
    "        color = 'green' if pred_cls == true_cls else 'red'\n",
    "        plt.title(f\"P:{pred_cls}\\nT:{true_cls}\", color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f\"val_epoch_{epoch:03d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 메인 함수\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    # [Transform 정의]\n",
    "    # 1. Cropped Data & Validation: 일반적인 Augmentation\n",
    "    tf_standard = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)), # 얼굴 위주 유지\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    # 2. Raw Data (Happy Train): 강제 줌인 (배경 제거 목적)\n",
    "    tf_raw = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.15, 0.6)), # 과감하게 확대\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    # 3. Validation: 정직한 CenterCrop\n",
    "    tf_val = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"--- Loading Datasets ---\")\n",
    "    # Train: Raw용과 Cropped용 Transform을 구분해서 전달\n",
    "    train_ds = MultiSourceEmotionDataset(\n",
    "        TRAIN_SOURCES, \n",
    "        transform_cropped=tf_standard, \n",
    "        transform_raw=tf_raw\n",
    "    )\n",
    "    \n",
    "    # Validation: Happy도 Cropped이므로 transform_cropped에 tf_val 전달 (raw는 None)\n",
    "    val_ds = MultiSourceEmotionDataset(\n",
    "        VAL_SOURCES, \n",
    "        transform_cropped=tf_val, \n",
    "        transform_raw=None \n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Creating Balanced Sampler...\")\n",
    "    train_weights = train_ds.get_weights()\n",
    "    sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = ResNetCBAMEmotion(num_classes=3).to(device)\n",
    "    \n",
    "    # [성공 요인 적용] Focal Loss 사용 (Happy 클래스 가중치 부여 가능)\n",
    "    # Happy 클래스(0번)에 조금 더 집중하도록 설정\n",
    "    class_weights = torch.tensor([1.2, 1.0, 1.0]).to(device) \n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    print(f\"--- Start Training for {NUM_EPOCHS} Epochs (ResNet+CBAM+Focal) ---\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss = 0.0\n",
    "        train_preds, train_trues = [], []\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, logits = model(imgs)\n",
    "            \n",
    "            loss = criterion(logits, lbls)\n",
    "            confs = confs.to(device)\n",
    "            loss = (loss * confs).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_trues.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_acc = (np.array(train_preds) == np.array(train_trues)).mean()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_preds, val_trues = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                val_loss_sum += criterion(logits, lbls).item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_trues.extend(lbls.cpu().numpy())\n",
    "        \n",
    "        val_acc = (np.array(val_preds) == np.array(val_trues)).mean()\n",
    "        f1_macro = f1_score(val_trues, val_preds, average='macro', zero_division=0)\n",
    "        f1_per_class = f1_score(val_trues, val_preds, average=None, zero_division=0)\n",
    "        \n",
    "        happy_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0.0\n",
    "        avg_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0.0\n",
    "        other_f1 = f1_per_class[2] if len(f1_per_class) > 2 else 0.0\n",
    "\n",
    "        logger.log([epoch+1, r_loss/len(train_loader), train_acc, val_loss_sum/len(val_loader), val_acc, f1_macro, happy_f1, avg_f1, other_f1])\n",
    "        \n",
    "        print(f\" -> Val Acc: {val_acc:.4f} | F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\" -> [Happy F1]: {happy_f1:.4f} | [Avg F1]: {avg_f1:.4f} | [Other F1]: {other_f1:.4f}\")\n",
    "        \n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR)\n",
    "        \n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Saved: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9905471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "--- Loading Datasets ---\n",
      "[Dataset] Loading images...\n",
      "  -> Found 1493 images in /workspace/merge_data/data/img/train/happy (Class: happy)\n",
      "  -> Found 1394 images in /workspace/user4/cropped/train/anger (Class: average)\n",
      "  -> Found 1368 images in /workspace/user4/cropped/train/panic (Class: other)\n",
      "  -> Found 1528 images in /workspace/user4/cropped/train/sadness (Class: other)\n",
      "[Dataset] Total: 5783 | Distribution: [1493 1394 2896]\n",
      "[Dataset] Loading images...\n",
      "  -> Found 299 images in /workspace/user4/cropped/val/happy (Class: happy)\n",
      "  -> Found 290 images in /workspace/user4/cropped/val/anger (Class: average)\n",
      "  -> Found 280 images in /workspace/user4/cropped/val/panic (Class: other)\n",
      "  -> Found 280 images in /workspace/user4/cropped/val/sadness (Class: other)\n",
      "[Dataset] Total: 1149 | Distribution: [299 290 560]\n",
      "[INFO] Creating Balanced Sampler...\n",
      "[INFO] Building ResNet50 + CBAM Attention Model...\n",
      "--- Start Training for 30 Epochs (Top-Crop + Strong Weight) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/30: 100%|██████████| 180/180 [00:55<00:00,  3.25it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.5683 | F1 Macro: 0.4273\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5566 | [Other F1]: 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2/30: 100%|██████████| 180/180 [00:53<00:00,  3.35it/s, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6014 | F1 Macro: 0.4435\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5695 | [Other F1]: 0.7610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3/30: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s, loss=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.5840 | F1 Macro: 0.4405\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5574 | [Other F1]: 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4/30: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s, loss=0.0694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6136 | F1 Macro: 0.4577\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5967 | [Other F1]: 0.7765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5/30: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s, loss=0.0672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6197 | F1 Macro: 0.4607\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6223 | [Other F1]: 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6/30: 100%|██████████| 180/180 [00:52<00:00,  3.41it/s, loss=0.0926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6136 | F1 Macro: 0.4568\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6210 | [Other F1]: 0.7494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7/30: 100%|██████████| 180/180 [00:53<00:00,  3.39it/s, loss=0.0573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6223 | F1 Macro: 0.4636\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6255 | [Other F1]: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8/30: 100%|██████████| 180/180 [00:55<00:00,  3.26it/s, loss=0.0435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6284 | F1 Macro: 0.4714\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6562 | [Other F1]: 0.7581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9/30: 100%|██████████| 180/180 [00:51<00:00,  3.51it/s, loss=0.0724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6275 | F1 Macro: 0.4695\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6371 | [Other F1]: 0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10/30: 100%|██████████| 180/180 [00:52<00:00,  3.42it/s, loss=0.0441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6284 | F1 Macro: 0.4687\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6350 | [Other F1]: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11/30: 100%|██████████| 180/180 [00:50<00:00,  3.54it/s, loss=0.0665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.5866 | F1 Macro: 0.4443\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6191 | [Other F1]: 0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12/30: 100%|██████████| 180/180 [00:51<00:00,  3.47it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.5944 | F1 Macro: 0.4472\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6022 | [Other F1]: 0.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13/30: 100%|██████████| 180/180 [00:52<00:00,  3.44it/s, loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6084 | F1 Macro: 0.4473\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.5690 | [Other F1]: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14/30: 100%|██████████| 180/180 [00:51<00:00,  3.52it/s, loss=0.0761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6171 | F1 Macro: 0.4575\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6008 | [Other F1]: 0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15/30: 100%|██████████| 180/180 [00:51<00:00,  3.49it/s, loss=0.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6292 | F1 Macro: 0.4781\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6955 | [Other F1]: 0.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16/30: 100%|██████████| 180/180 [00:53<00:00,  3.36it/s, loss=0.0543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6153 | F1 Macro: 0.4645\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6549 | [Other F1]: 0.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17/30: 100%|██████████| 180/180 [00:53<00:00,  3.37it/s, loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6188 | F1 Macro: 0.4646\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6510 | [Other F1]: 0.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18/30: 100%|██████████| 180/180 [00:53<00:00,  3.34it/s, loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6319 | F1 Macro: 0.4742\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6637 | [Other F1]: 0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19/30: 100%|██████████| 180/180 [00:53<00:00,  3.35it/s, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6249 | F1 Macro: 0.4698\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6599 | [Other F1]: 0.7496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20/30: 100%|██████████| 180/180 [00:51<00:00,  3.51it/s, loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6205 | F1 Macro: 0.4676\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6712 | [Other F1]: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 21/30: 100%|██████████| 180/180 [00:51<00:00,  3.49it/s, loss=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6214 | F1 Macro: 0.4656\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6407 | [Other F1]: 0.7562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 22/30: 100%|██████████| 180/180 [00:56<00:00,  3.21it/s, loss=0.045] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6353 | F1 Macro: 0.4792\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6868 | [Other F1]: 0.7509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 23/30: 100%|██████████| 180/180 [00:50<00:00,  3.56it/s, loss=0.0341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6345 | F1 Macro: 0.4772\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6771 | [Other F1]: 0.7544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 24/30: 100%|██████████| 180/180 [00:52<00:00,  3.44it/s, loss=0.0452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6258 | F1 Macro: 0.4690\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6504 | [Other F1]: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 25/30: 100%|██████████| 180/180 [00:53<00:00,  3.35it/s, loss=0.0344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6275 | F1 Macro: 0.4718\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6609 | [Other F1]: 0.7544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 26/30: 100%|██████████| 180/180 [00:53<00:00,  3.34it/s, loss=0.037] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6266 | F1 Macro: 0.4704\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6579 | [Other F1]: 0.7534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 27/30: 100%|██████████| 180/180 [00:53<00:00,  3.36it/s, loss=0.035] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6310 | F1 Macro: 0.4749\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6727 | [Other F1]: 0.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 28/30: 100%|██████████| 180/180 [00:51<00:00,  3.47it/s, loss=0.0369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6319 | F1 Macro: 0.4750\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6657 | [Other F1]: 0.7592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 29/30: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s, loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6310 | F1 Macro: 0.4739\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6657 | [Other F1]: 0.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 30/30: 100%|██████████| 180/180 [00:55<00:00,  3.22it/s, loss=0.0363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Val Acc: 0.6266 | F1 Macro: 0.4693\n",
      " -> [Happy F1]: 0.0000 | [Avg F1]: 0.6501 | [Other F1]: 0.7576\n",
      "[Done] Saved: /workspace/user5/runs_resnet_topcrop/emotion_model_resnet_topcrop.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# [중요] 손상된 이미지 로딩 허용\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "\n",
    "# 저장 경로 설정\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_resnet_topcrop\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_resnet_topcrop.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_resnet_topcrop.csv\"\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews\"\n",
    "TRAIN_PREVIEW_DIR = SAVE_DIR / \"train_previews\" # [추가] 학습 데이터 확인용\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_PREVIEW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 클래스 정의\n",
    "CLASS_TO_ID = {\"happy\": 0, \"average\": 1, \"other\": 2}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"average\", 2: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 데이터 소스\n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {  \n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/merge_data/data/img/train/happy\", # Raw Image\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": { \n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {   \n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\", \n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋 로직\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    extensions = [\"*.jpg\", \"*.JPG\", \"*.jpeg\", \"*.JPEG\", \"*.png\", \"*.PNG\", \"*.bmp\", \"*.BMP\"]\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files += list(p.rglob(ext))\n",
    "    return sorted(list(set(files)))\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform_cropped=None, transform_raw=None):\n",
    "        self.items = []\n",
    "        self.transform_cropped = transform_cropped\n",
    "        self.transform_raw = transform_raw\n",
    "        self.labels = [] \n",
    "\n",
    "        print(\"[Dataset] Loading images...\")\n",
    "        for label_group, sub_sources in sources.items():\n",
    "            if label_group not in CLASS_TO_ID: continue\n",
    "            class_id = CLASS_TO_ID[label_group]\n",
    "            \n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                print(f\"  -> Found {len(imgs)} images in {info['img_dir']} (Class: {label_group})\")\n",
    "                \n",
    "                # Raw 데이터 판별 로직\n",
    "                is_raw = False\n",
    "                # Happy 클래스이고 경로에 'cropped'가 없으면 Raw로 간주\n",
    "                if label_group == \"happy\" and \"cropped\" not in str(info[\"img_dir\"]):\n",
    "                    is_raw = True\n",
    "\n",
    "                for img in imgs:\n",
    "                    conf = 0.7 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf, is_raw))\n",
    "                    self.labels.append(class_id)\n",
    "        \n",
    "        self.class_counts = np.bincount(self.labels) if self.labels else np.array([0,0,0])\n",
    "        print(f\"[Dataset] Total: {len(self.items)} | Distribution: {self.class_counts}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf, is_raw = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # [전처리 분기]\n",
    "            if is_raw and self.transform_raw:\n",
    "                image = self.transform_raw(image)\n",
    "            elif self.transform_cropped:\n",
    "                image = self.transform_cropped(image)\n",
    "            else:\n",
    "                # Fallback\n",
    "                resize = transforms.Resize((224, 224))\n",
    "                image = resize(image)\n",
    "                image = transforms.ToTensor()(image)\n",
    "                \n",
    "            return image, label, conf\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def get_weights(self):\n",
    "        count_arr = np.maximum(self.class_counts, 1) \n",
    "        class_weights = 1.0 / count_arr\n",
    "        sample_weights = [class_weights[label] for label in self.labels]\n",
    "        return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "# =========================\n",
    "# 3) 모델 & Loss (Focal Loss)\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Label Smoothing 적용된 CE Loss 계산\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=0.1)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        return loss.sum()\n",
    "\n",
    "class ResNetCBAMEmotion(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ResNetCBAMEmotion, self).__init__()\n",
    "        print(\"[INFO] Building ResNet50 + CBAM Attention Model...\")\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        self.backbone = models.resnet50(weights=weights)\n",
    "        \n",
    "        self.feature_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        emb = self.projection(x)\n",
    "        logits = self.classifier(emb)\n",
    "        return emb, logits\n",
    "\n",
    "# =========================\n",
    "# 4) 유틸리티\n",
    "# =========================\n",
    "class CSVLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.header = [\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"macro_f1\", \"happy_f1\", \"avg_f1\", \"other_f1\"]\n",
    "        with open(self.filepath, mode='w', newline='') as f:\n",
    "            csv.writer(f).writerow(self.header)\n",
    "\n",
    "    def log(self, row):\n",
    "        with open(self.filepath, mode='a', newline='') as f:\n",
    "            csv.writer(f).writerow(row)\n",
    "\n",
    "def save_validation_preview(model, val_loader, epoch, device, save_dir):\n",
    "    model.eval()\n",
    "    try:\n",
    "        imgs, lbls, _ = next(iter(val_loader))\n",
    "    except: return\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        _, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    imgs_vis = torch.clamp(imgs * std + mean, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(16, len(imgs))):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        img_np = imgs_vis[i].cpu().permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img_np)\n",
    "        pred_cls = ID_TO_CLASS[preds[i].item()]\n",
    "        true_cls = ID_TO_CLASS[lbls[i].item()]\n",
    "        color = 'green' if pred_cls == true_cls else 'red'\n",
    "        plt.title(f\"P:{pred_cls}\\nT:{true_cls}\", color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f\"val_epoch_{epoch:03d}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# [추가] 학습 데이터가 어떻게 들어가는지 확인하는 함수\n",
    "def save_train_preview(imgs, lbls, epoch, save_dir):\n",
    "    if epoch > 1: return # 첫 에폭에만 저장\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    imgs_vis = torch.clamp(imgs.cpu() * std + mean, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(16, len(imgs))):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(imgs_vis[i].permute(1, 2, 0).numpy())\n",
    "        plt.title(f\"Train: {ID_TO_CLASS[lbls[i].item()]}\")\n",
    "        plt.axis('off')\n",
    "    plt.savefig(save_dir / f\"train_preview_ep{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 메인 함수\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    # [핵심 변경 1] 상단 Crop 함수 정의 (Lambda)\n",
    "    # 이미지의 상단 60%만 잘라내어 얼굴이 포함될 확률을 극대화\n",
    "    def crop_top_half(img):\n",
    "        w, h = img.size\n",
    "        # (left, top, right, bottom)\n",
    "        return img.crop((0, 0, w, int(h * 0.6)))\n",
    "\n",
    "    # [Happy Raw Data용 Transform]\n",
    "    # 상단 60% Crop -> 거기서 다시 Random Crop (줌인 효과)\n",
    "    tf_raw_happy = transforms.Compose([\n",
    "        transforms.Lambda(crop_top_half), # 다리/배경 제거\n",
    "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)), # 남은 상체에서 얼굴 찾기\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    # [Cropped Data용 Transform]\n",
    "    tf_standard = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    # [Validation용 Transform]\n",
    "    tf_val = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"--- Loading Datasets ---\")\n",
    "    train_ds = MultiSourceEmotionDataset(\n",
    "        TRAIN_SOURCES, \n",
    "        transform_cropped=tf_standard, \n",
    "        transform_raw=tf_raw_happy # Happy용 강력한 전처리 적용\n",
    "    )\n",
    "    \n",
    "    val_ds = MultiSourceEmotionDataset(\n",
    "        VAL_SOURCES, \n",
    "        transform_cropped=tf_val, \n",
    "        transform_raw=None \n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Creating Balanced Sampler...\")\n",
    "    train_weights = train_ds.get_weights()\n",
    "    sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = ResNetCBAMEmotion(num_classes=3).to(device)\n",
    "    \n",
    "    # [핵심 변경 2] Happy 클래스 가중치 극대화 (1.2 -> 3.0)\n",
    "    # Happy(0번)를 못 맞추면 Loss가 폭발하도록 설정\n",
    "    class_weights = torch.tensor([3.0, 1.0, 1.0]).to(device) \n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    logger = CSVLogger(CSV_PATH)\n",
    "\n",
    "    print(f\"--- Start Training for {NUM_EPOCHS} Epochs (Top-Crop + Strong Weight) ---\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        r_loss = 0.0\n",
    "        train_preds, train_trues = [], []\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for batch_idx, (imgs, lbls, confs) in enumerate(loop):\n",
    "            # [추가] 첫 배치를 이미지로 저장해서 Happy 데이터가 잘 잘리는지 확인\n",
    "            if batch_idx == 0:\n",
    "                save_train_preview(imgs, lbls, epoch+1, TRAIN_PREVIEW_DIR)\n",
    "\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, logits = model(imgs)\n",
    "            \n",
    "            loss = criterion(logits, lbls)\n",
    "            confs = confs.to(device)\n",
    "            loss = (loss * confs).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            r_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_trues.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_acc = (np.array(train_preds) == np.array(train_trues)).mean()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_preds, val_trues = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                _, logits = model(imgs)\n",
    "                val_loss_sum += criterion(logits, lbls).item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_trues.extend(lbls.cpu().numpy())\n",
    "        \n",
    "        val_acc = (np.array(val_preds) == np.array(val_trues)).mean()\n",
    "        f1_macro = f1_score(val_trues, val_preds, average='macro', zero_division=0)\n",
    "        f1_per_class = f1_score(val_trues, val_preds, average=None, zero_division=0)\n",
    "        \n",
    "        happy_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0.0\n",
    "        avg_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0.0\n",
    "        other_f1 = f1_per_class[2] if len(f1_per_class) > 2 else 0.0\n",
    "\n",
    "        logger.log([epoch+1, r_loss/len(train_loader), train_acc, val_loss_sum/len(val_loader), val_acc, f1_macro, happy_f1, avg_f1, other_f1])\n",
    "        \n",
    "        print(f\" -> Val Acc: {val_acc:.4f} | F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\" -> [Happy F1]: {happy_f1:.4f} | [Avg F1]: {avg_f1:.4f} | [Other F1]: {other_f1:.4f}\")\n",
    "        \n",
    "        save_validation_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR)\n",
    "        \n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"[Done] Saved: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c350fe",
   "metadata": {},
   "source": [
    "이미지happy척도가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7da70d",
   "metadata": {},
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# [중요] 손상된 이미지 로딩 허용\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# =========================\n",
    "# 0) 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4  \n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# 저장 경로 설정\n",
    "SAVE_DIR = Path(\"/workspace/user5/runs_efficientnet_final\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = SAVE_DIR / \"emotion_model_effnet.pt\"\n",
    "CSV_PATH = SAVE_DIR / \"results_effnet.csv\"\n",
    "IMG_SAVE_DIR = SAVE_DIR / \"val_previews\"\n",
    "IMG_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 클래스 정의\n",
    "CLASS_TO_ID = {\"happy\": 0, \"average\": 1, \"other\": 2}\n",
    "ID_TO_CLASS = {0: \"happy\", 1: \"average\", 2: \"other\"}\n",
    "\n",
    "# =========================\n",
    "# 1) 데이터 소스 \n",
    "# =========================\n",
    "TRAIN_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/new_data/label/happy_half.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/train/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/train/train_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_SOURCES = {\n",
    "    \"happy\": {\n",
    "        \"happy\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/happy\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_happy.json\"\n",
    "        }\n",
    "    },\n",
    "    \"average\": {\n",
    "        \"anger\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/anger\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_anger.json\"\n",
    "        }\n",
    "    },\n",
    "    \"other\": {\n",
    "        \"panic\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/panic\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_panic.json\"\n",
    "        },\n",
    "        \"sadness\": {\n",
    "            \"img_dir\": \"/workspace/user4/cropped/val/sadness\",\n",
    "            \"json_path\": \"/workspace/merge_data/data/label/val/val_sadness.json\"\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 2) 데이터셋 로직\n",
    "# =========================\n",
    "def list_images(img_dir: str) -> List[Path]:\n",
    "    p = Path(img_dir)\n",
    "    if not p.exists(): return []\n",
    "    extensions = [\"*.jpg\", \"*.JPG\", \"*.jpeg\", \"*.JPEG\", \"*.png\", \"*.PNG\"]\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files += list(p.rglob(ext))\n",
    "    return sorted(list(set(files)))\n",
    "\n",
    "class MultiSourceEmotionDataset(Dataset):\n",
    "    def __init__(self, sources: Dict, transform_cropped=None, transform_raw=None):\n",
    "        self.items = []\n",
    "        self.transform_cropped = transform_cropped\n",
    "        self.transform_raw = transform_raw\n",
    "        self.labels = []\n",
    "\n",
    "        print(\"[Dataset] Loading images...\")\n",
    "        for label_group, sub_sources in sources.items():\n",
    "            if label_group not in CLASS_TO_ID: continue\n",
    "            class_id = CLASS_TO_ID[label_group]\n",
    "            \n",
    "            for _, info in sub_sources.items():\n",
    "                imgs = list_images(info[\"img_dir\"])\n",
    "                print(f\"  -> Found {len(imgs)} images in {info['img_dir']} (Class: {label_group})\")\n",
    "                \n",
    "                # Raw 데이터 판별\n",
    "                is_raw = False\n",
    "                if label_group == \"happy\" and \"cropped\" not in str(info[\"img_dir\"]):\n",
    "                    is_raw = True\n",
    "\n",
    "                for img in imgs:\n",
    "                    # Weak Labeling 처리\n",
    "                    conf = 0.7 if \"weak\" in img.name.lower() else 1.0\n",
    "                    self.items.append((img, class_id, conf, is_raw))\n",
    "                    self.labels.append(class_id)\n",
    "        \n",
    "        self.class_counts = np.bincount(self.labels) if self.labels else np.array([0,0,0])\n",
    "        print(f\"[Dataset] Total: {len(self.items)} | Distribution: {self.class_counts}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, conf, is_raw = self.items[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if is_raw and self.transform_raw:\n",
    "                image = self.transform_raw(image)\n",
    "            elif self.transform_cropped:\n",
    "                image = self.transform_cropped(image)\n",
    "            else:\n",
    "                image = transforms.Resize((224, 224))(image)\n",
    "                image = transforms.ToTensor()(image)\n",
    "                \n",
    "            return image, label, conf\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error loading {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def get_weights(self):\n",
    "        # WeightedRandomSampler용 가중치 (각 샘플이 뽑힐 확률의 역수)\n",
    "        count_arr = np.maximum(self.class_counts, 1) \n",
    "        class_weights = 1.0 / count_arr\n",
    "        sample_weights = [class_weights[label] for label in self.labels]\n",
    "        return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "# =========================\n",
    "# 3) 모델: EfficientNet-B0\n",
    "# =========================\n",
    "class EfficientNetEmotion(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(EfficientNetEmotion, self).__init__()\n",
    "        print(\"[INFO] Building EfficientNet-B0 Model...\")\n",
    "        weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "        self.backbone = models.efficientnet_b0(weights=weights)\n",
    "        \n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Loss 함수\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=0.1)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        return loss.sum()\n",
    "\n",
    "# =========================\n",
    "# 4) 시각화 유틸리티 (수정됨)\n",
    "# =========================\n",
    "def save_epoch_preview(model, val_loader, epoch, device, save_dir):\n",
    "    model.eval()\n",
    "    try:\n",
    "        imgs, lbls, _ = next(iter(val_loader))\n",
    "    except StopIteration:\n",
    "        return\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf_values, preds = torch.max(probs, dim=1)\n",
    "\n",
    "    # [수정] Denormalize Broadcasting 오류 해결\n",
    "    # shape: (3,) -> (1, 3, 1, 1) 로 변환해야 (B, C, H, W)와 연산 가능\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    # 이미지 복원\n",
    "    imgs_vis = torch.clamp(imgs * std + mean, 0, 1)\n",
    "\n",
    "    cols = 4\n",
    "    rows = 4\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    for i in range(min(cols * rows, len(imgs))):\n",
    "        ax = plt.subplot(rows, cols, i+1)\n",
    "        img_np = imgs_vis[i].cpu().permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img_np)\n",
    "        \n",
    "        pred_cls = ID_TO_CLASS[preds[i].item()]\n",
    "        true_cls = ID_TO_CLASS[lbls[i].item()]\n",
    "        conf_val = conf_values[i].item()\n",
    "        \n",
    "        label_text = f\"P:{pred_cls}\\nT:{true_cls}\\n{conf_val:.2f}\"\n",
    "        \n",
    "        color_box = 'blue' if preds[i] == lbls[i] else 'red' # 틀리면 빨간색\n",
    "\n",
    "        plt.text(\n",
    "            x=5, y=5,\n",
    "            s=label_text,\n",
    "            color='white',\n",
    "            fontsize=9,\n",
    "            fontweight='bold',\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(facecolor=color_box, alpha=0.8, edgecolor='none', pad=2)\n",
    "        )\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f\"epoch_{epoch:03d}_preview.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 5) 학습 메인 함수\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "    \n",
    "    tf_raw = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 0.8)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    tf_crop = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    tf_val = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MultiSourceEmotionDataset(TRAIN_SOURCES, transform_cropped=tf_crop, transform_raw=tf_raw)\n",
    "    val_ds = MultiSourceEmotionDataset(VAL_SOURCES, transform_cropped=tf_val, transform_raw=None)\n",
    "\n",
    "    # 1. Sampler용 Weights (배치 단위 불균형 해소)\n",
    "    weights = train_ds.get_weights()\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = EfficientNetEmotion(num_classes=3).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    # 2. Loss Function용 Weights (모델 학습 시 불균형 해소)\n",
    "    # [수정] 데이터 비율의 역수를 계산하여 자동으로 할당\n",
    "    # [5245, 1394, 2896] -> Happy가 제일 많고 Anger가 제일 적음\n",
    "    # 가중치 = Max_Count / Class_Count\n",
    "    counts = train_ds.class_counts\n",
    "    max_count = np.max(counts)\n",
    "    calc_weights = max_count / (counts + 1e-6) # 0으로 나누기 방지\n",
    "    \n",
    "    # [1.0, 3.76, 1.81] 정도가 됨 -> Anger(Average)에 약 3.7배 가중치\n",
    "    class_weights = torch.tensor(calc_weights, dtype=torch.float32).to(device)\n",
    "    print(f\"[INFO] Calculated Class Weights: {class_weights}\") \n",
    "\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "\n",
    "    with open(CSV_PATH, 'w', newline='') as f:\n",
    "        csv.writer(f).writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\", \"f1_macro\", \"happy_f1\", \"avg_f1\", \"other_f1\"])\n",
    "\n",
    "    print(\"--- Start Training (EfficientNet-B0) ---\")\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for imgs, lbls, confs in loop:\n",
    "            imgs, lbls, confs = imgs.to(device), lbls.to(device), confs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, lbls)\n",
    "            \n",
    "            loss = (loss * confs).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        preds_list, trues_list = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls, _ in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, lbls)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                \n",
    "                preds_list.extend(preds.cpu().numpy())\n",
    "                trues_list.extend(lbls.cpu().numpy())\n",
    "\n",
    "        val_acc = (np.array(preds_list) == np.array(trues_list)).mean()\n",
    "        f1_macro = f1_score(trues_list, preds_list, average='macro', zero_division=0)\n",
    "        f1_per_class = f1_score(trues_list, preds_list, average=None, zero_division=0)\n",
    "        \n",
    "        happy_f1 = f1_per_class[0] if len(f1_per_class) > 0 else 0.0\n",
    "        avg_f1 = f1_per_class[1] if len(f1_per_class) > 1 else 0.0\n",
    "        other_f1 = f1_per_class[2] if len(f1_per_class) > 2 else 0.0\n",
    "\n",
    "        print(f\" -> Val Acc: {val_acc:.4f} | F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\" -> [Happy F1]: {happy_f1:.4f} | [Avg F1]: {avg_f1:.4f} | [Other F1]: {other_f1:.4f}\")\n",
    "\n",
    "        with open(CSV_PATH, 'a', newline='') as f:\n",
    "            csv.writer(f).writerow([epoch+1, train_loss/len(train_loader), val_loss/len(val_loader), val_acc, f1_macro, happy_f1, avg_f1, other_f1])\n",
    "        \n",
    "        save_epoch_preview(model, val_loader, epoch+1, device, IMG_SAVE_DIR)\n",
    "\n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\" -> Best Model Saved! (F1: {best_f1:.4f})\")\n",
    "\n",
    "    print(\"[Done] Training Finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf73d3",
   "metadata": {},
   "source": [
    "efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551159b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016e1eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: koreanize-matplotlib in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: matplotlib in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from koreanize-matplotlib) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib->koreanize-matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->koreanize-matplotlib) (1.17.0)\n",
      "Requirement already satisfied: torch in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: pandas in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (3.10.7)\n",
      "Requirement already satisfied: koreanize-matplotlib in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: pillow in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (12.0.0)\n",
      "Requirement already satisfied: filelock in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from triton==3.3.1->torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/miniconda3/envs/tomas/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install koreanize-matplotlib\n",
    "!pip install torch torchvision pandas matplotlib koreanize-matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04188df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib  # <-- 이 줄을 plt 사용 전에 넣어주세요\n",
    "\n",
    "plt.plot([1, 2, 3], [1, 4, 9])\n",
    "plt.title(\"한글 제목 - 자동 폰트 적용\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0625ba4",
   "metadata": {},
   "source": [
    "사용자성향 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fed0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [시스템] 표정 분석 엔진 구동 시작...\n",
      "\n",
      ">>> [분석] 감지된 사용자 반응 로그 (일부):\n",
      "  timestamp content_tag  emotion  happiness_score\n",
      "0     00:10        슬랩스틱  average         0.466206\n",
      "1     00:45       감동/눈물  average         0.211325\n",
      "2     01:20        사실정보  average         0.050000\n",
      "3     02:15        사회풍자    other         0.074984\n",
      "4     03:00         귀여움    happy         0.850000\n",
      "\n",
      ">>> [결과] 사용자 성향 도출: ENFP\n",
      ">>> [시스템] 시각화 보고서를 생성합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_569142/3544825502.py:185: UserWarning: Glyph 128522 (\\N{SMILING FACE WITH SMILING EYES}) missing from font(s) NanumGothic.\n",
      "  plt.tight_layout()\n",
      "/home/user5/miniconda3/envs/tomas/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128522 (\\N{SMILING FACE WITH SMILING EYES}) missing from font(s) NanumGothic.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# [중요] 스타일 설정을 import보다 먼저 하거나, 폰트 설정을 나중에 해야 합니다.\n",
    "# 1. 먼저 스타일을 지정해버립니다. (함수 안에서 뺌)\n",
    "# ==========================================\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "# ==========================================\n",
    "# 2. 그 다음 한글 폰트 라이브러리를 불러옵니다.\n",
    "# ==========================================\n",
    "import koreanize_matplotlib \n",
    "\n",
    "# [리눅스 안전장치] 만약 라이브러리가 있어도 안 될 경우를 대비해 나눔고딕 강제 지정\n",
    "# 리눅스에는 보통 'NanumGothic'이 설치되어 있습니다.\n",
    "plt.rcParams['font.family'] = 'NanumGothic' \n",
    "plt.rcParams['axes.unicode_minus'] = False # 마이너스 기호 깨짐 방지\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 이하 로직 동일\n",
    "# ==========================================\n",
    "USE_MOCK_DATA = True\n",
    "\n",
    "CLASS_MAP = {\"happy\": 0, \"average\": 1, \"other\": 2}\n",
    "idx_to_class = {v: k for k, v in CLASS_MAP.items()}\n",
    "\n",
    "# ... (Dataset, Model, Profiler 클래스는 기존과 동일하므로 생략하지 않고 그대로 둡니다) ...\n",
    "\n",
    "class FacialExpressionDataset(Dataset):\n",
    "    def __init__(self, sources, transform=None, is_mock=False):\n",
    "        self.sources = sources\n",
    "        self.transform = transform\n",
    "        self.is_mock = is_mock\n",
    "        self.data = []\n",
    "        if self.is_mock:\n",
    "            for _ in range(100):\n",
    "                label_str = random.choice(list(CLASS_MAP.keys()))\n",
    "                self.data.append((\"/mock/path/img.jpg\", CLASS_MAP[label_str]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        if self.is_mock:\n",
    "            image = torch.randn(3, 224, 224)\n",
    "        else:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "            except:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class LaughterProfiler:\n",
    "    def simulate_user_session(self):\n",
    "        # 데이터 생성 로직\n",
    "        content_log = [\n",
    "            {\"time\": \"00:10\", \"genre\": \"코미디\", \"tag\": \"슬랩스틱\", \"desc\": \"넘어지는 장면\"},\n",
    "            {\"time\": \"00:45\", \"genre\": \"드라마\", \"tag\": \"감동/눈물\", \"desc\": \"주인공의 재회\"},\n",
    "            {\"time\": \"01:20\", \"genre\": \"정보\", \"tag\": \"사실정보\", \"desc\": \"뉴스 속보\"},\n",
    "            {\"time\": \"02:15\", \"genre\": \"코미디\", \"tag\": \"사회풍자\", \"desc\": \"정치 풍자 농담\"},\n",
    "            {\"time\": \"03:00\", \"genre\": \"동물\", \"tag\": \"귀여움\", \"desc\": \"강아지 영상\"},\n",
    "            {\"time\": \"04:10\", \"genre\": \"공포\", \"tag\": \"반전개그\", \"desc\": \"귀신 등장 후 반전\"}\n",
    "        ]\n",
    "        session_data = []\n",
    "        for content in content_log:\n",
    "            if content['tag'] in ['귀여움', '반전개그']:\n",
    "                pred_probs = [0.85, 0.1, 0.05]\n",
    "            elif content['tag'] == '사실정보':\n",
    "                pred_probs = [0.05, 0.9, 0.05]\n",
    "            else:\n",
    "                pred_probs = np.random.dirichlet(alpha=[1, 1, 1]).tolist()\n",
    "            \n",
    "            predicted_class = np.argmax(pred_probs)\n",
    "            session_data.append({\n",
    "                \"timestamp\": content[\"time\"],\n",
    "                \"content_tag\": content[\"tag\"],\n",
    "                \"content_desc\": content[\"desc\"],\n",
    "                \"emotion\": idx_to_class[predicted_class],\n",
    "                \"happiness_score\": float(pred_probs[0])\n",
    "            })\n",
    "        return pd.DataFrame(session_data)\n",
    "\n",
    "    def analyze_mbti(self, df):\n",
    "        happy_moments = df[df['emotion'] == 'happy']\n",
    "        if happy_moments.empty:\n",
    "            return \"ISTJ또는 INTJ\", \"웃음 장벽이 에베레스트급! 냉철한 분석가 스타일입니다.\"\n",
    "        \n",
    "        avg_intensity = happy_moments['happiness_score'].mean()\n",
    "        tag_counts = happy_moments['content_tag'].value_counts()\n",
    "        top_tag = tag_counts.idxmax() if not tag_counts.empty else \"없음\"\n",
    "        \n",
    "        if \"반전\" in top_tag or \"풍자\" in top_tag: mbti_result = \"E-N성향\"\n",
    "        elif \"귀여움\" in top_tag or \"감동\" in top_tag: mbti_result = \"E-S성향\"\n",
    "        elif \"슬랩스틱\" in top_tag or \"예측불가능\" in top_tag: mbti_result = \"E-F성향\"\n",
    "        elif \"팩트폭격\"in top_tag or \"반전\" in top_tag: mbti_result = \"E-J성향\"\n",
    "        elif \"즉흥\" in top_tag or \"전염성\" in top_tag: mbti_result = \"E-P성향\"\n",
    "        else: mbti_result = \"극 대문자 I와 T성향이 강합니다.\"\n",
    "            \n",
    "        desc = f\"사용자님은 **'{top_tag}'** 요소에 가장 크게 반응했습니다.\\n(평균 웃음 강도: **{avg_intensity*100:.1f}%**)\"\n",
    "        return mbti_result, desc\n",
    "\n",
    "# ==========================================\n",
    "# 4. 시각화 함수 (여기가 핵심 수정됨)\n",
    "# ==========================================\n",
    "\n",
    "def generate_report_card(user_df, mbti_type, description):\n",
    "    # [수정] 함수 안에서 plt.style.use()를 호출하지 않습니다!\n",
    "    # 이미 맨 위에서 설정했거나, 여기서 호출하면 폰트가 초기화되기 때문입니다.\n",
    "    \n",
    "    # 혹시라도 폰트가 풀렸을까봐 한 번 더 강제 지정 (리눅스용)\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "    \n",
    "    # 1. 타임라인 차트\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    colors = ['tomato' if x > 0.9 else 'lightgray' for x in user_df['happiness_score']]\n",
    "    ax1.bar(user_df['timestamp'], user_df['happiness_score'], color=colors, alpha=0.8)\n",
    "    \n",
    "    ax1.set_title(\"시간대별 웃음 발생 구간 및 강도\", fontsize=15, fontweight='bold')\n",
    "    ax1.set_ylabel(\"기쁨(Happy) 확률\")\n",
    "    ax1.set_xlabel(\"영상 재생 시간\")\n",
    "    ax1.axhline(y=0.7, color='red', linestyle='--', alpha=0.3, label='폭소 임계점 (0.7)')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # 2. 성향 레이더 차트\n",
    "    categories = ['공감/감동', '지적 유머', '몸개그', '반전/스릴', '사회적 관계']\n",
    "    if \"F\" in mbti_type: values = [5, 2, 4, 3, 5]\n",
    "    elif \"T\" in mbti_type: values = [1, 5, 2, 4, 2]\n",
    "    else: values = [3, 3, 3, 3, 3]\n",
    "    \n",
    "    values += values[:1]\n",
    "    angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[1, 0], polar=True)\n",
    "    ax2.plot(angles, values, linewidth=2, linestyle='solid', color='tomato')\n",
    "    ax2.fill(angles, values, 'tomato', alpha=0.2)\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories, fontsize=11)\n",
    "    ax2.set_title(\"웃음 유발 요소 분석\", pad=20, fontweight='bold', fontsize=13)\n",
    "    ax2.set_yticks([]) \n",
    "    \n",
    "    # 3. 최종 결과 텍스트\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    recommends = \"• 추천: 예상을 뒤엎는 '반전 스릴러'\\n• 추천: 마음이 따뜻해지는 '동물 농장'\"\n",
    "    if \"T\" in mbti_type:\n",
    "        recommends = \"• 추천: 지적 호기심을 채우는 '과학 예능'\\n• 추천: 블랙코미디 시트콤\"\n",
    "\n",
    "    text_str = (\n",
    "        f\"😊 당신의 웃음 성향 리포트 😊\\n\\n\"\n",
    "        f\"판정 유형: >> {mbti_type} <<\\n\\n\"\n",
    "        f\"{description}\\n\\n\"\n",
    "        f\"[맞춤형 콘텐츠 추천]\\n{recommends}\"\n",
    "    )\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='#FFEFD5', alpha=0.5, edgecolor='orange')\n",
    "    ax3.text(0.5, 0.5, text_str, transform=ax3.transAxes, fontsize=12,\n",
    "            verticalalignment='center', horizontalalignment='center', bbox=props)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# [MAIN] 실행부\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\">>> [시스템] 표정 분석 엔진 구동 시작...\")\n",
    "    \n",
    "    profiler = LaughterProfiler()\n",
    "    user_session_df = profiler.simulate_user_session()\n",
    "    \n",
    "    print(\"\\n>>> [분석] 감지된 사용자 반응 로그 (일부):\")\n",
    "    print(user_session_df[['timestamp', 'content_tag', 'emotion', 'happiness_score']].head())\n",
    "    \n",
    "    mbti_type, description = profiler.analyze_mbti(user_session_df)\n",
    "    \n",
    "    print(f\"\\n>>> [결과] 사용자 성향 도출: {mbti_type}\")\n",
    "    \n",
    "    print(\">>> [시스템] 시각화 보고서를 생성합니다...\")\n",
    "    generate_report_card(user_session_df, mbti_type, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd1919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
